\section{Project Management}
\label{section:project_management}
This chapter reflects organizational aspects.
We will compare relevant propositions from the project plan to the actual state \cite{projectplan}.
First, we talk about time management. Then, quality aspects will be discussed.

\subsection{Time Management}
This chapter shows the time spent and how well we were able to stick to the planned schedule.
The times are taken from Redmine \cite{redmine}.

\subsubsection{Time Spent From Each Student}
The total working hours for the bachelor thesis was defined to be 12 ECTS or 360 hours per student \cite{projectplan}.
The project lasted 17 weeks, which averages out in a nominal working time of 21.2 hours per week per student \cite{projectplan}.
During the last two weeks, no other classes took place and more time resources were available \cite{projectplan}.
The submission of the thesis was brought forward to Wednesday, the 10th of June 2020 instead of the following Friday.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{time/time_tracking}
    \caption{Time Tracking}
    \label{fig:timetracking}
\end{figure}

As shown in figure \ref{fig:timetracking}, each student spent about 363 hours in total.
In the last two weeks, a significant increase in workload can be seen. \\

Figure \ref{fig:timetracking_activity} shows the invested time by activity.
The largest part was development, followed by documentation and meetings.
We rate this basic time split as well taken.
For testing, a relatively small percentage was spent according to the diagram.
Since the boundary between implementation and testing is not so clear for test driven development,
time was sometimes rather reported for development than for testing.

\begin{figure}[H]
    \centering
    \includegraphics[width=11cm]{time/activity}
    \caption{Time by Activity}
    \label{fig:timetracking_activity}
\end{figure}

Figure \ref{fig:time_milestones} shows the accumulated time per milestone.
The individual milestones are described in more detail in the following section.
It is interesting how little time the transition took and how time-consuming the symbol table was.

\begin{figure}[H]
    \centering
    \includegraphics[width=12.5cm]{time/milestones}
    \caption{Time by Milestone}
    \label{fig:time_milestones}
\end{figure}

Apart from the time-consuming meetings and documentation, the symbol table was the most intensive ticket.
This is followed by the newly implemented client.
Figure \ref{fig:time_ticket} shows the six most time consuming tickets.

\begin{figure}[H]
    \centering
    \includegraphics[width=13cm]{time/top6}
    \caption{Top 6 Time Consuming Tickets}
    \label{fig:time_ticket}
\end{figure}

Figure \ref{fig:time_feature} shows the reported time per core feature.
The numbers are to be enjoyed with caution.
A lot of time spent on features was also reported on dedicated tickets for testing or refactoring.
Nevertheless, the graphic gives an overview of which features
were more time-consuming and which were easier to implement.

\begin{figure}[H]
    \centering
    \includegraphics[width=10cm]{time/feature.png}
    \caption{Time for Each Feature}
    \label{fig:time_feature}
\end{figure}

\subsubsection{Schedule and Scope}
As planned in the project plan, we used a mix between unified process and SCRUM.
The project was split into an inception, elaboration, construction and a transition stage
as you can see in figure \ref{fig:project_staging}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{project_staging}
    \caption{Project Staging}
    \label{fig:project_staging}
\end{figure}

Each of the stages was split into sprints lasting one week.
This is shown in figure \ref{fig:planned_milestones}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{milestones_planned.png}
    \caption{Planned Milestones}
    \label{fig:planned_milestones}
\end{figure}

In general, we were able to stick to this plan pretty well as you can see in figure \ref{fig:actual_milestones}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{milestones_actual}
    \caption{Actual Milestones}
    \label{fig:actual_milestones}
\end{figure}

{\bf Inception}\\
In the first two weeks, we found our way back into the project from the prototype
and set up our environments accordingly.
This included updating the IDEs, OmniSharp, Z3 and Boogie,
as well as setting up a new Redmine ticket system and create the project plan.\\

{\bf Infrastructure}\\
Since the majority of the inception stage could be processed very quickly,
we were able to start with parts of the infrastructure a week earlier.
On the one hand, we got the old CI running again with the new updated versions of Boogie and Dafny,
and on the other hand, the CI was extended by SonarQube for \Csharp.
This also included splitting the client and server projects into different git repositories.\\

In addition, we have revised the concept for integration tests and developed
a working test-prototype.

Completing the CI process had been somewhat delayed, but was then successfully finished.\\

{\bf Improvements}\\
In this milestone, we took care to improve the existing features.
Only features that are not related to the symbol table were dealt with.
This included:
\begin{itemize}
    \item Compilation
    \item CounterExample
    \item Verification
\end{itemize}

Small improvements of the client were also done, such as a bug fix in the status bar.
This milestone could be completed on time.\\

{\bf Symbol Table}\\
At the beginning of this milestone,
we developed the concept and the basic implementation for the symbol table.
A prototype with support for a few AST-Nodes could be achieved very quickly.
However, implementing support for the massive amount of AST-Nodes took very long.\\

We started to already make use of the symbol table before the symbol table was complete.
This was done to gain a break from the tedious visitor implementation.\\

{\bf Extensions}\\
As described above, this milestone was started a little earlier.

The content of this milestone was primarily to make the existing features
- such as \textit{AutoCompletion} and \textit{CodeLens} -
use the new symbol table.
In addition to the improvement of the existing features, we also
added new features like \textit{HoverInformation} and \textit{Rename},
which are also based on the new symbol table.\\

{\bf Transition}\\
The transition stage was primarily based about preparing our plugin for the release.
We noticed relatively early that there were problems with the support for Mono.
Therefore, we already made rough clarifications in week ten and wrote posts in appropriate forums,
namely the OmniSharp slack channel \cite{mono-slack}.\\

The transition took a little longer, since the the plugin had to be finalized before it could be published.
Instead, we started a bit earlier with writing the documentation.\\

{\bf Conclusion}\\
The conclusion was basically used to write the documentation and as a general buffer time.
The buffer time was not necessary, since we respected our plan and showed enough courage to leave out some visitor functionality in order to be on time.
The buffer could then be used to do some finalization work and prepare the presentation.\\

\subsection{Effects of the COVID-19 Pandemic}
The cooperation was only marginally affected by the Corona situation.
Physical meetings were replaced by virtual video conferences and screen sharing.
This was done for the meetings with the supervisors, but also for internal meetings of the two students.

\subsubsection{Meetings}
The weekly meetings with our supervisors Thomas Corbat and Fabian Hauser were held in general each Thursday at 10:30am \cite{projectplan}.
Thomas Kistler and Marcel Hess had a reserved time frame of 3x8h from Wednesday to
Friday each week to work on the project \cite{projectplan}.
Whenever necessary, internal meetings were held within this time frame \cite{projectplan}.

\subsubsection{Division of Labour}
Thomas Kistler was more involved in the realisation of the symbol table,
while Marcel Hess was working a bit more on the client implementation.
However, both were always involved in both concepts.
Basic design implementations were therefore mostly implemented in pair programming.\\

In the implementation of core providers, both participated equally.
To enable a precise tracking of the activities and also to facilitate collaboration,
we worked with the version management git.

\subsection{Quality Aspects}
This chapter highlights certain quality aspects that have been crucial for good code quality.
In general, the requirements placed in the project plan were fulfilled \cite{projectplan}.

\subsubsection{Code Reviews}
There were three layers of non-automated code quality control.
\begin{enumerate}
    \item Internal code reviews of the students. Critical and difficult parts of the code were always underlaying to a internal code review.
    \item Reviews with the supervisors
    \item The four-eye principle using merge requests.
\end{enumerate}


Since GitLab supports merge requests,
merge requests for independent changes were sent to the project partner for control and approval.
If no errors or improvement potential was found, the merge request was accepted.
If there were minor issues, they were improved immediately.
In the case of major discrepancies, a code review was convened.

\subsubsection{Continuous Integration (CI)}
According to the project plan,
we wanted to resolve all CI-issues at the beginning of the bachelor thesis,
so that we can then profit by a supportive workflow \cite{projectplan}. \glsadd{ci}
This goal could be achieved.
By separating the client and server into two different repositories, two new CI pipelines were created.
These are then discussed for the server and for the client. \\

{\bf Client}\\
The client goes through three pipeline phases as shown in figure \ref{fig:ci_client}:
\begin{enumerate}
    \item \textit{Test}
    \item \textit{Build}
    \item \textit{Sonar}
\end{enumerate}

\begin{figure}[H]
    \centering
    \includegraphics[width=10cm]{ci_pipelines_client}
    \caption{CI Pipelines for the Client}
    \label{fig:ci_client}
\end{figure}

In the test phase, prettier \cite{dev} is used to check if the TypeScript code
is formatted according to the style guidelines \cite{projectplan}.
The build pipeline checks whether the TypeScript code can be compiled without errors.
If there are errors in the TypeScript or warnings - such as an unused variable - the build will fail.
Finally, the scanner for SonarQube is run and the report is sent to SonarCloud. \\

{\bf Server}\\
The pipeline for the server was slightly modified.
In order to create the SonarQube report, the build and Sonar scan processes had to be merged
as shown in figure \ref{fig:ci_server}.

\begin{figure}[H]
    \centering
    \includegraphics[width=8cm]{ci_pipelines_server}
    \caption{CI Pipelines for the Server}
    \label{fig:ci_server}
\end{figure}

After the build has been created and the SonarQube report has been successfully submitted to SonarCloud,
the test phase begins.
Existing Dafny tests are run to verify
that we have not inadvertently changed anything in the Dafny project,
and our own unit and integration tests are executed in the \code{test\_nunit} stage. \\

The automated CI processes were a great enrichment for our work.
The tests were automatically run for checked-in changes.
If these failed, no merge request was created.
This automated control ensured that a new bug was not accidentally overlooked
and that the quality standard was maintained.

\subsubsection{Static Code Analysis}
Aside SonarQube, ReSharper was used locally for the server to gain additional insights.
Regarding the client, SonarLint was used to get a local, static code analysis \cite{sonar-lint}.
The fact that a static code analysis could already be carried out locally meant that errors
could be already corrected during development and were not only noticed after the CI had failed.

SonarQube was used for the server and for the client within the CI process.
To integrate SonarQube into our CI process, we use the SonarCloud as a platform \cite{sonarcloud}.
This is shown in figure \ref{fig:sonarcloud_report}.


\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{sonar_report_client_server}
    \caption{SonarQube Report in SonarCloud}
    \label{fig:sonarcloud_report}
\end{figure}

It stands out that there is a high number of code smells and bugs.
These high numbers are due to the fact that the entire Dafny project is analyzed by Sonar.
The actual numbers for our project are of course much lower,
as shown in figure \ref{fig:sonar_server_detail}.
The \code{DafnyLanguageServer} is highlighted in yellow.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{sonar_server_detail.png}
    \caption{SonarQube Report in SonarCloud for the Dafny project}
    \label{fig:sonar_server_detail}
\end{figure}

Furthermore, local plugins for VSCode and Visual Studio were used to automatically
format the code according to our styling guides \cite{dev}.
Thus our code was always automatically formatted uniformly. \\

More detailed code metrics can be found in section \ref{section:results:metrics}.

\subsubsection{Test Coverage}
Since our integration tests are able to test all components of the language server
and the client contains basically no logic,
tests were written only for the Dafny language server.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{coverage/overview}
    \caption{Language Server Test Coverage}
    \label{fig:test_coverage}
\end{figure}

The test coverage can be seen in figure \ref{fig:test_coverage}.
We did not achieve a full 100\%, but still a very high percentage was reached.
Components that contain no important logic remain untested.
For example, code fragments that call Dafny itself and only pass information were not specifically tested, since no advantage would be gained.
Listing \ref{lst:nottested} shows another code fragment that pulls down test coverage.
It is taken from the \code{DeclarationVisitor}.

\pagebreak

\begin{lstlisting}[language=typescript, caption={Untested Code}, captionpos=b, label={lst:nottested}]
public override void Visit(ThisExpr e) {
    throw new InvalidOperationException(Resources.ExceptionMessages.visit_only_declarations);
}

public override void Leave(ThisExpr e) {
    throw new InvalidOperationException(Resources.ExceptionMessages.visit_only_declarations);
}

public override void Visit(DisplayExpression o) {
    throw new InvalidOperationException(Resources.ExceptionMessages.visit_only_declarations);
}

public override void Leave(DisplayExpression o) {
    throw new InvalidOperationException(Resources.ExceptionMessages.visit_only_declarations);
}

public override void Visit(ComprehensionExpr o) {
    throw new InvalidOperationException(Resources.ExceptionMessages.visit_only_declarations);
}

public override void Leave(ComprehensionExpr o) {
    throw new InvalidOperationException(Resources.ExceptionMessages.visit_only_declarations);
}

...
\end{lstlisting}

The \code{DeclarationVisitor} is not visiting any of these AST-nodes (the \code{DeepVisitor} is doing that),
yet it has to provide the methods for them because it is inheriting from the base class \code{Visitor}.
However, whenever core logic takes place, the code is tested.
Thus, we are well satisfied with the test coverage of more than 85\%.