\section{Implementation}
This chapter describes the implementation of the planned design.

\subsection{Client}
The implementation of the client could be done as intended.
The whole code could be broken down into the individual components defined in the design phase.

\subsubsection{Better Encapsulation Through Interfaces and Modules}
During the implementation process, many confusing module imports and component usages arose.
Their purpose was generally unclear and thus difficult to understand.
The imports caused a lot of dependencies, too.

This problem had to be solved by further introductions of interfaces and encapsulation of modules. \\

Although interfaces were used for individual types,
core components did not use their own interfaces.
To reduce coupling, isolated modules were formed in a comprehensive refactoring process.
The modules now no longer program on the class implementations, but against the interface. \\

For this purpose one importable module with the name \code{\_<Directory>Modules} was created for each directory.
Figure \ref{fig:client_2nd_refactoring} shows an overview of the interfaces.
In addition, the dependencies among each other are shown.
For simplicity, the contents of \code{stringRessources} and \code{typeInterfaces} have been omitted. \\

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{client_2nd_refactoring.png}
    \caption{Second Major Client Refactoring}
    \label{fig:client_2nd_refactoring}
\end{figure}

At first glance, the architecture appears much tidier.
The dependencies are now pointing from top to bottom.
Methods have been simplified and the number of parameters could be reduced significantly.
Component identifiers have been renamed to be more understandable. \\

However, it is now also noticeable that there are considerably more dependencies on \code{stringRessources}.
While in the previous version only the module \code{ui} used \code{stringResources}, it is now used by almost all other modules.\\

This has the following reason: Up until this refactoring, the task of \code{stringResources} was to be a central collection of all UI strings. \glsadd{UI} \intnote{is das glossar? dann msüste es ja eig midnetsens ui anzeigen :O}
In the code review, it was decided that default values should no longer be set within the independent modules,
but rather at a central location.
This would make it easier to maintain these values.

\subsubsection{Encapsulation of the VS Code Components}
 \intnote{einfache migration auf andere IDE ermoeglicht}

die exportierten module:
bei einer migration müssen einfach die vscode module durch die module der neuen ide beim export ersetzt werden.
die exportierten modulnamen wurden daher extra neutral gewählt.

für bestimmte komponenten können wrapper klassen als adapter geschrieben werden, falls von
der anderen/neuen IDE keine äquivalente klasse zur verfügung steht.

dies  wird durch dieses prinzip der kapselung ermöglicht.
programierer für neeu IDE muss fürs neue Plugin lediglich dieses eine modul anpassen.
der languageserver sowie die restlichen client komponenten kännen dann 1.1 verwendet werden.

davon ausgenommen sind natürlich allgemeine plugin veröffentlichungs konfigurationen, wie konkret
das package.js von visual studio code.

disclaimer: natürlich muss die plugin entwicklung für die ide typescirpt supporten.
ist das der fall? welche sprache verwendet eclipse, atom (java.. auch ts?),
war eklipse nicht java? dahatten wir doch was in frameworks bei mirko mal.

\subsubsection{Download of the Dafny Language Server}
\label{section:implementation:client:download}
 \intnote{GitLab Pages} (Fabian gm Meeting... hust erwaehnen)
Interfaces geschrieben, versucht API zu nehmen, ging ned also server
dank interface austauschbar
interface altem tool nachempfunden. teils namen passender gewaehlt.

beschreiben interface download, wie das ausgetaucht werden kann.

\subsubsection{Customizable Through Configuration}
 \intnote{config} - sollte neu dann auch noch durch gereiht werden oder über vscode kapselung zur verfügung gestellt werden
man kann alles konfigurieren. support fuer dark mode.,
counter example  - light und dark mode.. custom colors.
compile (args)
sonstige config und so sachen wurden zentralisiert. gm code review.



todo hier nochmals jedes modul einzeln aufzeigen, was die aufgabe ist, welche interfaces und komponenten nach aussen exportiert werden
=> oder ist das überflüssig? hmm... keine überschneidungen mit dem design / analyse
tom: ich hab dasselbe probelm biem server. weiss auch net recht wie damti umgehen. ich liess es jetzt mal durch, mal sehen ev lösch ich vieels weg.... 
ich glaub aam besten so machen: 'konnte wieg epalnt gemacht werden, z.b. dies das wurde so umgesetzt: und dann ein stück code das schön ist oder so'
nachfolgend werden die einzelnen module aufgezeigt.
die grün dargestellten teile des moduls werden nach aussen exportiert und können von anderen modulen verwendet werden.
die gelben/roten/orange teile werden nicht nach aussen exportiert und sind innerhalb des moduls gekapselt.

interface bild vom design als abgespeckte variante hier nochmals einbinden oder nicht? ka mal gucken. eher weniger wens 1.1 das gleiche ist. braucht unnötig platz. wär aber gut zur einleitung / lesefluss.

Sonderheut vom DOenload und dessen interface gleich hier in den einzelnen beschreibungen aufnehmen (Sprich im modul vom server beschreiben!!) dann haben wir das auch gleich erledigt.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Server}
In this section, the server implementation is discussed.
Just as in the Design-chapter, the server structure will be analyzed from top to bottom, starting from the \code{Main}-method and ending at the utility layer.
The implementation of the symbol table is split into a separate section.

\subsubsection{Server Launch}
The server starts by executing the \code{Main} method.
As done in common practice, it is kept very short.
All it does is launching the language server, which is already handled by another class.
The full \code{Main}-method is shown in listing \ref{lst:mainserver}.

\begin{lstlisting}[language=csharp, caption={Main Function}, captionpos=b, label={lst:mainserver}]
public static async Task Main(string[] args)
{
    DafnyLanguageServer languageServer = new DafnyLanguageServer(args);
    await languageServer.StartServer();
}
\end{lstlisting}

The launch of the server itself is divided into four stages.
First, preparational work is done.
This happens already in the constructor of the language server.
Preparation includes
\begin{itemize}
    \item Reading and processing config variables
    \item Setting up the logging framework
\end{itemize}
Secondly, the actual server is launched.
The logger will directly be injected and all handlers are registered.
In the third stage, once the server is running, a message sending service is instantiated to notify the client about the successful server start and, if any, errors occurred during start up.
Lastly, the console output stream is redirected to keep the language server stream isolated.
The constructor and the \code{StartServer}-method are partially shown in listing \ref{lst:serverstart}.

\begin{lstlisting}[language=csharp, caption={Starting the Language Server}, captionpos=b, label={lst:serverstart}]
public class DafnyLanguageServer{
    public DafnyLanguageServer(string[] args)
    {
        var configInitializer = new ConfigInitializer(args);
        configInitializer.SetUp();
        configInitErrors = configInitializer.Errors;
        log = LoggerCreator.GetLogger();
    }
        
    public async Task StartServer()
    {
        log.Debug(Resources.LoggingMessages.server_starting);
        server = await LanguageServer.From(options => options
            .WithHandler<TextDocumentSyncTaskHandler>()
            .WithHandler<RenameTaskHandler>()
            ...
        );
        ExecutePostLaunchTasks();
        await RedirectStreamUntilServerExits();
        log.Debug(Resources.LoggingMessages.server_closed);
    }
}
\end{lstlisting}

\subsubsection{Handler}
Handlers are passed to the language server and are called whenever the language server receives a corresponding request.
Services, such logging or workspace management, can be injected and are thus available for the handler.
As discussed in chapter \ref{chapter:design}, handlers are based around a \code{Handle} method.
For example, every time the server receives a \code{textDocument/Definitions} request, this \code{Handle} method will be called.
The parameter and return types are specific per request.
GoToDefinition would pass a text document location as input, namely the cursor position, and it expects a \code{LocationLink} as response,
namely where the cursor should jump to.
Own requests can be realized according to section \ref{chapter:customlspmsg} by defining an own interface.\\

All handlers require two additional methods, aside the actual \code{Handle}:
\begin{itemize}
    \item GetRegistrationOptions: This method is called when the handler is registered.
    It allows to set options at the time the server is started.
    Such an option is for example after which characters AutoCompletion should be automatically triggered.
    \item SetCapability: It allows to set handler-specific capabilities, such as if the rename-handler will also support a 'prepare rename' feature.
\end{itemize}

A lot of code for these classes is always identical and was thus extracted to a generic base class.
This concerns for example the creation of a logger out of the logger factory or handling of errors.\\

To keep all of this separated from the actual tasks, it was targeted to just forward the request to a core-provider.
This could be well achieved.
Many handlers look just as shown in listing \ref{lst:handlecompilation}.

\begin{lstlisting}[language=csharp, caption={Handling Compilation}, captionpos=b, label={lst:handlecompilation}]
public async Task<CompilerResults> Handle(CompilerParams request, CancellationToken cancellationToken)
{
   _log.LogInformation(string.Format(Resources.LoggingMessages.request_handle, _method));
   try
   {
       FileRepository f = _workspaceManager.GetFileRepository(request.FileToCompile);
       return await Task.Run(() => f.Compile(request.CompilationArguments), cancellationToken);
   }
   catch (Exception e)
   {
       HandleError(string.Format(Resources.LoggingMessages.request_error, _method), e);
       return null;
   }
}
\end{lstlisting}

The file to compile is given as an argument.
The corresponding repository can be requested from the \code{WorkspaceManager}, which is injected to all handlers.
The request is then forwarded to the according provider, which will calculate the results.
All information required, such as the precompiled program, are available from within the \code{FileRepository}.
In case of an error, a message is sent to the user and the error is logged within the \code{HandleError} method.\\

Some handlers take additional actions, such as awaiting the result of the provider, and then sending user feedback according to the outcome.
This is done with GoToDefinition for example.
If the request was triggered at a declaration, an additional user message is sent.
By handling all communication with the handler, the message sending service does not have to be passed downwards to the core logic component.


\subsubsection{Core}
Within this package, the result of a request is assembled.
While it may sound like complex logic is happening here, it turned out that for most cases, all information is just available by the symbol table.
Consider the example of GoToDefinition.
A code excerpt is shown in listing \ref{lst:gotoProvierExample}.
As you can see, the actual task of finding the definition is resovled by the symbol table engine and just available as a property.
The actual task just considers the assembly of the correct result wrapper class.

\begin{lstlisting}[language=csharp, caption={GoToDefinition, Core Provider}, captionpos=b, label={lst:gotoProvierExample}]
ISymbol symbolAtCursor = _manager.GetSymbolAtPosition(uri, line, col);
ISymbol originSymbol = symbolAtCursor.DeclarationOrigin;

var position = new Position((long)originSymbol.Line - 1, (long)originSymbol.Column - 1);
var range = new Range { Start = position, End = position };
var location = new Location { Uri = originSymbol.FileUri, Range = range };

List<LocationOrLocationLink> result = new List<LocationOrLocationLink>();
result.Add(new LocationOrLocationLink(location));
return new LocationOrLocationLinks(result);
\end{lstlisting}

There are a few features that had to be extended with more complex logic.
These are described in section \ref{section:imp:features}

\subsubsection{Workspace}
The workspace is a component representing any opened files by the client.
Thus, it naturally consists only of a single property.
This is a dictionary, mapping a file-location to an internal file-representation:

\begin{lstlisting}[language=csharp, caption={Workspace Property}, captionpos=b, label={lst:workspaceproperty}]
private readonly ConcurrentDictionary<Uri, FileRepository> _files;
\end{lstlisting}

It offers methods to retrieve files and to update them.
Since updates can be down in two different kinds, incremental or full, the update method is overloaded for both cases.
The update requests are forwarded to the class \code{FileRepository}, which is used as the internal representation of a file.
It of course contains the source code.
However, this is not done directly as a string property, but wrapped in a class \code{PhysicalFile}.
Thus, the actual representation on the hard disk is separated even further.
The \code{PhysicalFile} class can then also take responsibility for applying file updates.\\

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{implementation/workspace.png}
    \caption{WorkspaceManager and FileRepository}
    \label{fig:worksapceAndRepo}
\end{figure}

Aside the file content, each \code{FileRepository} will also contain \code{TranslationResults}.
\code{TranslationResults} is a wrapper class for anything provided by the Dafny backend:
\begin{itemize}
    \item Could the file be parsed?
    \item Could it be verified?
    \item Is it logically correct?
    \item What errors and warnings occurred?
    \item How far could it be compiled?
    \item What internal compilation results could be produced for later reuse?
\end{itemize}

Last but not least, the newly implemented symbol table is also attached to the file repository.
To obtain all of these results, the class simply invokes the \code{SymbolTableGenerator} and the \code{DafnyTranslationUnit}.
Thus, all information about a file is accessible from within the file repository.


\subsubsection{DafnyAccess}
Dafny Access it the package invoking the Dafny backend to obtain verification results.
The core class in this package is the \code{DafnyTranslationUnit}.
It was partially taken over from the pre-existing projects, but as part of this bachelor thesis it was refactored and simplified.\\

\begin{figure}[h]
    \centering
    \includegraphics[width=12cm]{implementation/DTUSeq.png}
    \caption{WorkspaceManager and FileRepository}
    \label{fig:DTUSeq}
\end{figure}

In the constructor, the translation unit accepts a \code{PhysicalFile}.
The class then offers a single public method \code{public TranslationResults Verify()}.
Within the method, the following sequence of events as illustrated in figure \ref{fig:DTUSeq} occurs:

\begin{enumerate}
    \item It is checked that the instance has never been used before.
    This is, since the error reporter must be empty.
    Otherwise, errors would be reported multiple times.
    \item Next, Dafny is configured.
    This includes the registration of the Dafny error reporter and setting any options to default.
    The only non-default option is that the engine is supposed to generate a model file, which can later be used for counter example calculation.
    The configuration is shown in \ref{lst:setupdafnyoptions}.
    \item The Dafny parser is called.
    This step will report any syntax errors.
    \item The Dafny Resolver is called, if parsing was successful.
    This step will do semantic checks, such as type checks.
    A \code{DafnyProgram} results upon success.
    \item If successful, the precompiled \code{DafnyProgram} will be split into \code{BoogieProgram}s.
    \item The Boogie Execution Engine is invoked to perform logical correctness checks on the \code{BoogieProgram}s.
    \item Any errors that were reported are collected, converted and provided in the field \code{\_diagnosticElements}
    \item All results are wrapped by the \code{TranslationResult} class, providing the diagnostics, the Dafny program and the boogie programs.
    Also, within the property \code{TranslationStatus}, it is remarked how far the verification and translation process succeeded.
\end{enumerate}


Note in the following snippet, that dafny options are by directly calling Dafny's backend.
\begin{lstlisting}[language=csharp, caption={Setting up Dafny Options}, captionpos=b, label={lst:setupdafnyoptions}]
    private void SetUpDafnyOptions()
    {
        DafnyOptions.Install(new DafnyOptions(_reporter));
        DafnyOptions.Clo.ApplyDefaultOptions();
        DafnyOptions.O.ModelViewFile = FileAndFolderLocations.modelBVD;
    }
\end{lstlisting}





\subsubsection{Utilitites}
The bottom layer of the project is formed by the utiltiy layer.
It contains three packages
\begin{itemize}
    \item Tools
    \item Commons
    \item Resources
\end{itemize}
\code{Commons} contains classes that are used within multiple parts of the code, such as \code{TranslationResults} and were already discussed.
\code{Resources} contains string resources and were realized with \code{.resx} files.
Within the \code{Tools} package,
a variety of services can be found that do not necessarily directly correspond to Dafny,
but are useful within the language server environment.\\

\textbf{Config Initializer}\\
This class is used prior to the server launch and initializes a few config settings.
The settings are stored within the static class \code{Commons/LanguageServerconfig.cs}.
The config initializer will first of all set hard coded default values to avoid any kind of null pointer exceptions.
Afterwards, the file \code{Config/LanguageServerConfig.json} is parsed with Newtonsoft's Json.NET library \cite{jsondotnet}.
Any available values will be written to the static configuration class.
Unknown or illegal values will not be set and errors are added to an error reporter.
Finally, the launch arguments are parsed, again overwriting the config settings if applicable or reporting errors otherwise.
A simple argument parser was implemented manually.
Alternatively, a library could have been used for this task such as \cite{clparser}.
The config initializer is implemented exception safe.
This will run to completion and at worst just provide default values.
Errors can later be extracted from the \code{ErrorReporter}.
Each task is done by a dedicated component.\\

\textbf{LoggerCreator}\\
This class simply sets up a \textit{Serilog} \cite{serilog} logger.
For this purpose, the following information is extracted from the \code{LanguageServerConfig}:
\begin{itemize}
    \item Minimum loglevel
    \item Path of the logfile
\end{itemize}

\textbf{MessageSenderService}\\
This is a simple class accepting a \code{ILanguageServer} in the constructor.
Afterwards, it provides methods to send notifications to the client.
Similar to logging, methods for each severity level are available, such as \code{public void SendError(string msg)}.\\

\textbf{ReservedWordsProvider}\\
This is a class providing a set of words, that are not suited for identifiers.
This is, for example, 'method', 'class', or 'return'.
The class tries to read and parse \code{Config/ReservedDafnyWords.json}, which can be user adjusted in case the Dafny specification changes.
If the file cannot be read or has a wrong format, a hard coded default list is used which was taken out from the Dafny Reference Manual \cite{dafnyReferenceManual}.
A HashSet was used to provide fast access to the set.\\

While this component is specifically used solely for the \code{rename}-Feature, it was extracted to be also available at other spots if required for future features.



%%%%FUCK IST DAS LANG!!!!





%%%%%%%%%%%%%%%%%%%%TOMS WORK IN PROGRAESS HIER WEIT_ER MARKER/&&&&%%%%%%%%%"*çTQFGAWZHAZAZE%HRAZEHAEZ%RAEZ%



\subsection{Symbol Table}
This chapter describes the implementation of the symbol table.
The symbol table was designed to handle four different tasks:

\begin{itemize}
    \item Providing Symbol Information
    \item Symbol Table Generation
    \item Symbol Table Navigation
    \item Symbol Table Management
\end{itemize}

\subsubsection{Symbol Information}
This is a component that summarizes all information about a symbol.
Logic was extracted from this class as far as possible to keep it short and concise.
As we have seen in chapter \ref{chapter:design}, this class needs to contain a lot of properties.
The most important properties are:
\begin{itemize}
    \item Name
    \item File
    \item Position in File
    \item Body Location, if any
    \item Kind and Type
    \item Link to Parent Symbol
    \item Link to Declaration Symbol
    \item Hash with all Children Symbols (Only Declarations)
    \item List with all Descendants (Any symbol occurring in the body)
    \item List with all Usages of the symbol
    \item BaseClasses
    \item Parameters
    \item The associated module
    \item Link to the associated default class for quick access.
\end{itemize}

The provided properties are supposed to facilitate working with the symbol table.
For example, if a symbol's definition cannot be found with regular methods, the property with the associated default class can be accessed to search the symbol there.
No separate logic to find the default class is necessary.
This is advantageous, since the default class is in the global namespace and not a direct ancestor of a symbol.\\

Technically, the symbol table is more of a tree, then a table.
The data structure is double linked.
Each symbol knows about its descendants, but also about its ancestor.
Navigating to either one can thus be done in $O(1)$.
If the name of a descendant is known, navigating to the symbol can also be done in $O(1)$ due to the hash map.
For example, if a module \code{M} contains a class \code{C}, and within the class there is a method \code{foo}, one can simply start from the root symbol and navigate through the hash maps.
The \code{[]} operator was overloaded to make this as convenient as possible:\\
\code{rootSymbol["M"]["C"]["foo"]}.\\

To enable efficient access to the children of a symbol, we have opted for a key-value data structure.
The key is the child symbol's name, the value the actual \code{SymbolInformation} object.
This hash structure enables access to a child symbol with a runtime of $O(1)$.
Since every symbol also has a link to it's parent, navigation in both ways can be done within $O(1)$.\\

While this is very fancy, the convenience comes at a price.
Many properties do not apply for all kinds of symbols.
Consider the following code segment:

\begin{lstlisting}[language=dafny, caption={Example Code Regarding Symbol Information}, captionpos=b, label={lst:aldbkajds}]
method foo() {
    var x := 5;
    print x;
}
\end{lstlisting}

The symbol \code{foo} profits by almost all properties.
It can have children (the variable x), it has some parent, it can be used.
Since foo is a method declaration, the declaration property of the symbol does not make sense.
In this case, it actually just points to itself.
The declaration of \code{x} in the second line will have many null values within the symbol information.
While the parent is \code{foo}, it can not have any children.
Since it is a variable definition, it can have usages though.
The final usage of \code{x} in the last line (which we also consider a symbol), does not even have usages, since it is a usage itself.
Thus many properties are just null for that symbol.\\

Unused properties are just set to \code{null}.
This causes the risk of \code{NullPointerExceptions}, but should save a lot of memory compared to empty lists.\\

\subsubsection{Symbol Table Creation}
The symbol table generator accepts a precompiled Dafny program in the constructor.
The generator offers a public method \code{GenerateSymbolTable}.
It will first of all create a virtual root symbol.
Any other symbols will be attached to the root node as descendants.
The root node is also the final return value.\\

Then, all modules (similar to \CsharpWithSpace or C++ namespaces) will be extracted out of the Dafny program.
The modules will be sorted by depth, so that top level modules will be treated first and nested modules can be attached properly later on.\\

Once the modules are sorted by depth, the algorithm iterates over each of the modules.
The proper parent symbol will be extracted.
For a top level module, this is just the root symbol.
Otherwise, for nested modules, the parent module will be located.
Finally, the module will accept the declaration visitor.
The visitor is described later.
Once it has completed, all symbol declarations are registered in the symbol table.
A second iteration is then started, using a visitor, which will ignore declarations but run through all method bodies and take care of symbol usages.\\
This way, symbols that are declared after the first usage can be found as well.\\

\textbf{Visitor}\\
As already mentioned, the whole symbol table generation is realized using the visitor pattern.
For that, Dafny code had to be adjusted to offer a \code{Accept(Visitor v)} method.
This method will basically just navigate through the internal Dafny symbol representation.
For example, when visiting a method, one would like to register the method itself.
This is done by the expression \code{v.Visit(this)}.
However, the method also contains a Dafny \code{ensures} statement, which may contain further variables.
Thus, all statements within the \code{ensures} clause have to be visited.
The \code{Accept}-method will now just forward the call, using \code{foreach (var e in this.EnsureStatements) {e.Accept(v)}}.
The same applies for method parameters and other items like the requires clause.
Finally, the body of the method is to visit using \code{foreach (var stmt in this.Body) {stmt.Accept(v)}}.
Once everything is done, the scope of the method is left by calling \code{v.Leave(this)}.\\

If you recall the last section, it was said that two runs are performed.
One to capture all declarations, and one to visit all method bodies.
Thus, the visitor is having a boolean property \code{GoesDeep}, which decides if method bodies are visited or not.
The final \code{Accept} method for a method looks as shown in listing \ref{lst:visitoraccept}.
The method is shortened, there are more clauses like for example the requires clause.


\begin{lstlisting}[language=csharp, caption={Accepting a Visitor}, captionpos=b, label={lst:visitoraccept}]
public override void Accept(Visitor v)
{
  v.Visit(this);
  if (v.GoesDeep)
  {
    foreach (var ens in this.Ens)
    {
      ens.Accept(v);
    }
    foreach (var stmt in this.Body.Body)
    {
      stmt.Accept(v);
    }
  }
  v.Leave(this);
}
\end{lstlisting}

Note that the method is marked with the override keyword.
This is, since every AST-Element is either a statement or an expression, among others.
In case a specific AST element is not treated separately, a general accept method is defined for statements, as well as expressions.
However, for supported AST elements, a more specific method is supposed to be used.\\

On the other hand of the acceptor, there is the actual visitor.
The visitor has to implement \code{Visit} methods for each of the AST elements that it is supposed to visit, for example our \code{Method} from the preceding example.\\

The Visitor itself will now actually build up the symbol table.
For that, it stores the current scope in a property.
For example, when visiting a method, the parent scope is always some kind of class that was visited before.
The visitor will now create a symbol for the method, and the property 'parent' can just be set with the scope the visitor has stored - the class mentioned before.
Since the method itself will have it's own body, the new scope can then be set to the method.
All symbols that will be visited afterwards - which is anything inside the method - are then attached to the method.
Once the method is done, \code{Leave()} is called, which will reset the scope to the parent class for the next item to be visited.\\

The \code{Visit}-method in listing \ref{lst:visitorvisit1} of the first visitor, which only takes care of declarations, will just create a symbol, and attach it to the proper scope.
All required information is taken from the AST element that is visited.
This includes the name, the position, and so on.\\

\begin{lstlisting}[language=csharp, caption={Visiting a Method}, captionpos=b, label={lst:visitorvisit1}]
public override void Visit(Method o)
{
    var symbol = CreateSymbol(
        name: o.Name,
        kind: Kind.Method,

        positionAsToken: o.tok,
        bodyStartPosAsToken: o.BodyStartTok,
        bodyEndPosAsToken: o.BodyEndTok,

        isDeclaration: true,
        declarationSymbol: null,
        addUsageAtDeclaration: false,

        canHaveChildren: true,
        canBeUsed: true
    );
    SetScope(symbol);
}
\end{lstlisting}

The \code{CreateSymbol} method will set all properties accordingly.
That means, a symbol that can have children will be initialized with a list for children, while a symbol that cannot have children will just have a null entry there.
Note that the scope of the visitor is set to the method for future visitations.\\

The second visitor will also visit declarations, but no longer create a symbol for them.
Instead, the already created symbol is located and set as the proper scope.
Once a method body is encountered, it will now also be visited.
Within the body, local variables exist.
Despite local variables being declarations, they were not handled by the first visitor, which is actually responsible for declarations.
However, this is fine, since local variables are not accessible before they were not declared.
Furthermore, symbol usages such as method calls or variable usages are now encountered.
The visitor has to create proper symbols for these.
Since these are symbol usages, it is not sufficient to just create a symbol and attach it to the parent scope.
The following additional tasks have to be done:
\begin{itemize}
    \item Where is the symbol declared?
    \item Add usage to the symbol's declaration
\end{itemize}

To find the declaration, the symbol table navigator can already be used.
The navigator will just iterate from parent to parent and will return the first symbol, that is a declaration and matches the name.
Challenges occurred when a symbol is defined in global scope or in a inherited base class.
Both difficulties were resolved by adding separate checks for them.
Once the declaration is found, it is simple to add the just newly created symbol as one of the declaration's usages.\\

The main challenge of the whole visitor implementation was to find a symbols declaration.
The task could be extracted to the navigator as seen in listing \ref{lst:visitorfinddecl}.

\begin{lstlisting}[language=csharp, caption={Finding a Declaration}, captionpos=b, label={lst:visitorfinddecl}]
protected ISymbol FindDeclaration(string target, ISymbol scope, Kind kind)
{
    INavigator navigator = new SymbolTableNavigator();
    bool filter(ISymbol s) => s.Name == target && s.IsDeclaration && s.Kind == kind;
    return navigator.BottomUpFirst(scope, filter);
}
\end{lstlisting}

This snippet assigns the navigator to move upwards to tree, starting from the scope the visitor is just operating in.
In every scope, the navigator will iterate over all symbols and search one, that is a declaration and matches in name and kind.
For example, if we start inside a method, every symbol already visited in that method will be checked.
If no matching declaration was found, the navigator will move to the parent of the method, which must be a class.
Then, all symbols within that class are searched.
Now, the navigator may find a class method that actually matches the search criteria.
Since all declarations were registered beforehand, the process works even if the symbol is declared after the usage, unless it is a local variable, where it must be declared anyway before usage.\\

If the symbol cannot be found, the algorithm will search the default scope at the end as a last option to locate the symbol.
Since the symbol tree is moved upwards, the process takes $O(logn)$ time.
It is executed for every occurring symbol, thus the building of the symbol table takes $O(nlogn)$ time, while n denoted the amount of occurring symbols.\\

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{implementation/findDeclaration.png}
    \caption{Finding a Declaration using the Navigator}
    \label{fig:findDecl}
\end{figure}


\intnote{ZwischenfragE: Ist der ganze Bums hier zu lang?????? ist ja eewig hoy shit wtf}

\subsubsection{Symbol Table Navigator}
To operate on the (partially) constructed symbol table, a separate component to navigate was created.
It has basically two procedures.
Remember that the data structure of the symbol table is basically a double linked tree.

\begin{itemize}
\item TopDown: Starting from a node, the navigator dives downwards and searches a specific symbol.
\item BottomUp: Starting from a node, the navigator moves upwards the tree and searches a specific symbol, as seen in the previous section.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{tmpPlaceHolderTodo}
    \caption{TopDown (left) and ButtomUp (right) Visualized}
    \label{fig:impl_symboltablenav}
\end{figure}

Both options are implemented so that they can return a single, first match, or any symbols that match a criterion.
To illustrate this, two examples are shown in figure \ref{fig:impl_symboltablenav}.
With this type of tree inspection, the runtime is reduced, as it is no longer necessary to visit every symbol.

For the special case that the symbol table tree for a Dafny program was built very flat
(a Dafny program, which only uses the default scope with a main method)
the worst case runtime is still \code(O(n)) as you can see in figure \ref{fig:impl_symboltablenav_o_of_n}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{tmpPlaceHolderTodo}
    \caption{Dafny Code (left) Which Generates a Flat Symbol Table (right)}
    \label{fig:impl_symboltablenav_o_of_n}
\end{figure}

\textbf{What symbol is at the current cursor position?}\\
\code{TopDown} is called.
The rootSymbol and the current cursor position are handed as arguments.
The rootSymbol has 3 modules attached to it.
One ranging from line 1 to 20, another ranging from line 21 to 40.
The algorithm will now decide, in which of the two modules a further search is worth wile.
If the cursor is located at line 25, it will continue to search in the second module.
This can simply be done by calling the same function recursively, handing the second module as the entry point for the recursive search.
Within the recursive call, the proper class will be found, and so on.
Default namespaces and default classes had to be treated separately for this case.
Since the tree is moved along a single branch, the runtime for this process is $O(logn)$.\\

\textbf{What auto completion suggestions are indicated?}\\
To build up autocompletion suggestions, all declarations available at a certain locations must be figured.
First of all, the previous \code{TopDown} algorithm is used to find the current location.
Then, all declarations in the current scope are requested.
Afterwards, the algorithm moves to the parent scope, and will again search for all declarations.
This process is repeated until the root scope is reached.
Again, the problem can be solved using recursion.
This algorithm is also visiting just a single tree branch, thus requiring $O(logn)$ time.\\

The navigator offers further methods to visit the whole tree if that is desired by the caller.
The method \code{TopDownAll} will visit all branches, not just the one wrapping a certain location.
Consequently, \code{TopDownAll} requires $O(n)$ runtime and should not be used if possible.\\



\textbf{Symbol Table Manager}\\
The manager is a rather simple component and can be used as an access point to perform operations on the symbol table.
It is constructed with a root symbol of a fully generated symbol table and contains a navigator.
It then offers methods such as \code{ISymbol GetSymbolByPosition(Uri file, int line, int character)}.
That method will use the navigator to provide the user with the desired result.
Access to the rather complex navigator is encapsulated this way.



\subsection{Features}
\label{section:imp:features}
As it was stated in the previous sections, many features just query the symbol table and assemble the information to a proper result format.
This is especially the case for Rename, GoToDefinition and HoverInformation.
These features are thus not described in detail.
However, some features, especially AutoCompletion, involve a bit more complexity.
In this chapter, these special cases are described.

\subsubsection{Completion}
\label{section:implementation:core:completion}
The automatic code completion is an essential feature of every IDE and is a great help for the developer to write code efficiently.
Making good suggestions to the user is a complex matter. \\

Through the implementation of our new symbol table, it is easy for us to find symbols in the current scope.
With the help of the "Symbol Table Navigator"
it is also possible to efficiently filter the proposals by conditions,
which can be transferred as delegates. \\

The main difficulty and thus the logical complexity of the auto completion
is therefore no longer the actual collection of symbol suggestions,
but the evaluation of what kind of suggestions the user wants in the current context
and extracting a symbol as entry point for the symbol table navigator component. \\

For our auto completion, we support three fundamentally different types of user desire.
To find out the user's desire, the following approach is taken.
First the cursor position is used to determine the current code position in the Dafny source file.
Then it is checked if there is a dot in front of the cursos position,
or a \code{new} keyword followed by a space was found.
Depending on whether one of the two triggers was found,
a decision is made between the three user desires, as shown in figure \ref{fig:get_users_desire}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{tmpPlaceHolderTodo}
    \caption{Evaluating Users Desire - PAP/Ablaufsdiagramm/Flussdiagramm}
    \label{fig:get_users_desire}
\end{figure}

\textbf{Proposals for the Object Instance}\\
If a dot was found at the position of the curser, the word before the dot is also extracted from the code line.
The position of the cursors is then used to determine the entry.

The symbol table is searched using the navigator:
the symbol that encloses the current cursor position is searched.
For example, if the user is typing in the method \code{myMethod}, the symbol for \code{myMethod} is returned. \\

The symbol table is then searched for the extracted string,
which was located before the dot, using the found entry point.
The symbol that has exactly the right name is searched.
During the search, the navigator starts in the innermost scope and then moves up one level until a suitable symbol is found. \\

If a matching symbol is found, the corresponding symbol is now
for example the object instance of a class named \code{myClass}.
Using the references in the symbol table, the symbol of the class can be accessed from the instance.
All methods and variables are now returned as auto completion suggestions. \\

For the sake of clarity, the described process was visualized in the following
Dafny code example \intnote{xxx
in figure xx} as a sequence diagram.

\begin{lstlisting}[language=dafny, caption={tmptodo.dfy}, captionpos=b, label={lst:tmptodo}]
class myClass {
   var ABC: int;
   constructor () { }
   method myMethod() { /* do something */ }
}
method Main() {
   var abc := new ClassC();
   abc.
}
\end{lstlisting}

todo snipped in IDE screenshotten, cleich auch die resultate listen als screenshot rechts davon

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{tmpPlaceHolderTodo}
    \caption{Sequence Diagram for Object Instances}
    \label{fig:object_completion_diagram}
\end{figure}

\textbf{Class Suggestions}\\
If it is assumed that the user only wants classes proposed, the following approach is followed.

As with the proposals for an object instance,
the completion provider first looks for an access symbol via the cursor position.
The symbol that encloses the current cursor position with the smallest span is used as the entry symbol.
This symbol is found by the symbol navigator
Then iteration is performed from the inner scope
to the outermost scope and all symbols found are returned as as completion suggestions.
The list is already filtered during iteration by a passed delegate for class symbols only.

 \intnote{todo habe ich jeweils prädicate oder delegate geschrieben für die filterung? auch beim result iwo) }

 \intnote{todo wird irgend wo beim visitor manager dingens beschrieben, dass zu oberst dann nochmals runter ins default package und in die default klasse gegeagen wird um die "nicht benannten" symbole / default scope zu holen? }

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{tmpPlaceHolderTodo}
    \caption{Sequence Diagram for Class Suggestions}
    \label{fig:new_completion_diagram}
\end{figure}

\textbf{All Available Symbols as Default}\\
If it is assumed that the user would like to have all symbols proposed in the current context,
the procedure is basically the same as for the proposals for class suggestions.
The only difference is that no filter predicates are passed to the symbol navigator. Therefore, all types of symbols are suggested.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{tmpPlaceHolderTodo}
    \caption{Sequence Diagram for all Symbols in Scope}
    \label{fig:default_completion_diagram}
\end{figure}

\code{todo für alle implementationen: Ablauf. Diagramme wie in der SA sind nützlich. das mag "name einsetzen" bestimmt. \\
PAP oder flussdiagramm zeichnen für die fall untrscheidungen\\
das vom papier digitalisieren todo\\}

\textbf{Automatic Insertion of Brackets}\\
Since in Dafny round brackets have to be set when calling a method or initializing a class,
these brackets are automatically inserted as a user help
if a symbol of type method or class is selected in auto completion. \\

 \intnote{todo dies konnte realisiert werden mit xxx von LSP. }
Whenever a symbol of type class or method is added to the list of auto completion suggestions,
an opening and closing bracket is added to the text to be inserted.

 \intnote{todo beschreiben mit welchem parameter das im symbol information vom completion gemacht werden konnte. namen un dinsert text iwas. todo nur für dot completion oder für alle methoden u klassen? }

\textbf{Insert Parameter Placeholders}\\
From certain IDE's such as XCode,
you are used to seeing placeholders for the parameters and their types in auto completion
when you select a method from the suggestions \cite{sa}.

That is why we wanted to implement such a convenient feature for our auto completion as well. \\

Unfortunately we had to realize that the VS Code API
does not offer a comparable function that we could simply call,
as was the case with CodeLens, for example \cite{vscodeAPI}.

Nor does the LSP support a standard for the explosive insertion of parameters \cite{lspspec}. \\

That there is no simple way to implement this feature does not mean that it cannot be implemented.
But the effort for the implementation in VS Code would be exceptionally high.
Therefore we have decided not to support this feature.
The idea is described below. \\

As with CodeLens, you could send additional data to the client
for the auto completion objects - namely the parameters and their types for methods.
Whenever a completion is inserted, a callback function can be called within the client.
This function could then additionally insert the parameters as text in the code and register
event listeners on keyboard keys like \textit{TAB}.
Using \textit{TextEdits}, the registered function would then jump to the next placeholder parameter
each time \textit{TAB} is pressed and mark it, so that the user can easily replace it with his own code.
This logic is relatively complex and the exact VS Code API calls to it have not been researched. \\

Also, just because VS Code does not yet offer an API to perform such \textit{TAB} jumps between parameters,
does not mean that other IDE's, for which Dafny support will be offered later,
do not offer such a function.
In the future, VS Code might even add such a function. \\

\textbf{Getting Information About Method Parameters} \\
\intnote{hier ev noch was zu updaten aka ist eig einfach verfügbar blabla?}
Since the desired implementation of placeholder parameters was not easy to implement,
we have generally neglected the information for parameters.

However, the usability test has shown that text information about which method
receives which parameters would be very helpful for users.
Offering such assistance is possible with relatively little effort.
However, the structure of the symbol table has to be slightly modified,
which is why we have not included this feature in the functional scope for time reasons,
as there would have been no time for the necessary unit tests.

Nevertheless, in the following the idea is recorded to enable other developers to implement this feature.\\

Already now in our visitor, all sub symbols of a symbol are stored as an array within the symbol
as \code{List<ISymbol> Children} when building the symbol table.
For example, a method stores all contained variable declarations as an array of child symbols. \\

When building the symbol table,
the visitor must now be minimally adjusted so that parameters
of a method are now also visited and stored in an array.
That is it - afterwards the parameters of an auto-completion method are available. \\

When constructing the completion elements,
the information field \code{CompletionItem} can contain besides
the type of the symbol now the parameters to be passed as help text in \code{Detail} too. \\

This function is of course not only limited to methods but can also be applied to class constructors.

\subsubsection{Code Lens}
The CodeLens function is divided into two primary tasks.

Firstly, the grey help text in the IDE maintenance of methods and classes
shows how often the method or class has already been used.

Secondly, the user can click on the gray help text and a popup window displays all code snippeds
that use the method or class.
This allows the developer to efficiently jump between different places within a file
and across multiple files without having to switch between the files themselves or scroll much within a file. \\

\textbf{Number of References} \\
To get the display,
which methods and classes were used how often and thus also to mark dead code
as it is displayed in figure \ref{fig:codelens_show_ref_number},
no big effort is necessary thanks to the new symbol table.\\

Via the symbol manager all declarations for the currently opened file a.
Subsequently, the uses are counted for each symbol.
This is stored in the symbol table for each symbol in the list \code{Usages}. \\

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{tmpPlaceHolderTodo}
    \caption{CodeLens shows Number of Counted Referances}
    \label{fig:codelens_show_ref_number}
\end{figure}

\textbf{Popup Windows with Code Snippeds} \\
The popup that opens when you click on the grey text
with the counted references, as shown in figure \ref{fig:codelens_show_popup},
is basically a client feature offered by VS Code \cite{vscodeAPI}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{tmpPlaceHolderTodo}
    \caption{CodeLens shows Popup with Code Snippeds}
    \label{fig:codelens_show_popup}
\end{figure}

In the client the logic is minimal.
Only the function \code{VSCodeCommandStrings.ShowReferences},
offered by VS Code, is called \cite{vscodeAPI}. \\

The complete logic and the preparation of the data is already done in the language server.
Basicly for each symbol declaration, the CodeLens object is enriched with further information
as shown in the following code snipped as listing \ref{lst:codelens-prep}.

\begin{lstlisting}[language=csharp, caption={LSP Handler Implementation}, captionpos=b, label={lst:codelens-prep}]
var args = new
{
    Uri = _uri,
    Position = position, // where to show the CodeLens popup
    Locations = locations // what should be in the pop up displayed
};
Command command = new Command
{
    Title = msgTitle,
    Name = "dafny.showReferences",
    Arguments = new JArray(JsonConvert.SerializeObject(args))
};
return new CodeLens
{
    Data = _uri,
    Range = range,
    Command = command
};
\end{lstlisting}
 \intnote{todo merke grad dass showreferences ein übersehenes magic string ist... in ressourcen auslagern? todo}

For each code lens element, for example a method, a \code{Command} object is also appended.
The stored \code{Name} within this \code{Command}
is the function name called when at the client the \code{Command}
(which is the actual grey text with the reference information) is clicked. \\

In addition, the \code{Command} is given the required reference positions as arguments.
These reference positions are referenced in the CodeLens popup as code snippeds.  \\

If the function \code{dafny.showReferences} is called in the client,
the method \code{VSCodeCommandStrings.ShowReferences} from the VS Code API is called
and the arguments prepared in sevrer are passed. \\

Because the whole logic for CodeLens is in the server,
integration tests and unit tests can be written on the server side for this feature.\\

\textbf{Support for File Includation} \\
CodeLens basically works very well with regard to support over multiple files.
But only if other files are included.
In case the currently opened file is included by another file,
CodeLens does not know that the included symbols are used by a third Dafny program. \\

This problem should solve itself once a global symbol table for the complete workspace has been implemented.
Symbols then also know about uses and references of the whole workspace.
More information about the global symbol table is described in section xxx.
 \intnote{todo XXX mit Kapitelreferenz auf updatebare globale symbol tabelle manager dingens tun. seh grad ned welche skapitel es ist. haben wir das schon geschrieben? todo}

\subsubsection{Compilation}
Every time a file gets updated, the whole Dafny backend is triggered and the results are stored in the \code{FileRepository}.
This includes the precompiled \code{dafnyProgram}.
The compilation provider will take advantage of that and use the precompiled item, and just forward it to the compiler engine.
Prior to the compilation, custom compilation arguments are installed, if the user provided any.
The process is fully integrated into the Dafny backend by using the following code line.\\

\begin{lstlisting}[language=csharp, caption={Calling the Dafny Compiler}, captionpos=b, label={lst:dafnycompiler}]
DafnyDriver.CompileDafnyProgram(dafnyProgarm, filePath, otherFiles, true, textwriter);
\end{lstlisting}

The provider will check if any errors occurred and return the outcome within a wrapper class.

\subsubsection{Counter Example}
This feature bases on the model file, which is generated during verification.
The model file is a key-value store generated by Boogie.
It contains several states.
Each state tracks the content of variables during different stages of the proof.
Of interest is primarily the \textit{initial} state, since this one contains the information how variables need to be set at the initially to achieve a counter example.\\

This provider reads the model file and uses the Boogie backend to convert into a useful format.
Afterwards, it extracts the initial state from the model.
Out of the remaining model, all key-value-pairs containing useful information are extracted, assembled and returned.
The component will also transform information into a more human readable format, e.g. $((- 12)) \rightarrow -12$.
Furthermore, many values are internal Boogie references and cannot be resolved.
These just look like \code{T@U!val!12}.
Such values are replaced with the text \code{[Object Reference]}.\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Mono Support for macOS and Linux}
\label{section:implementation:mono}
One of the core objectives was to provide support for multiple platforms.
This means that in addition to Windows, the platforms macOS and Linux should be also supported. \\

In our previous thesis we had to switch from .Net Core to .Net Framework \cite{sa}.
The reason for this was that although OmniSharp was compatible as a Language Server .Net core,
the Dafny core project was not \cite{sa}.
Due to dependency problems, we also switched the new Dafny Language server to .Net Framework \cite{sa}. \\

.Net core would have been easily supported by Linux and macOs.
With .Net Framework, however, an \code{.exe} is generated as program compilation.
Therefore you have to rely on mono on Unix based operating systems.
Mono allows to execute \code{.exe} files on said systems \cite{mono}. \\

Unfortunately, in the course of our thesis we discovered that the Language server could not be started correctly with mono.
The problem already occurred when trying to start the OmniSharp language server component. \\

We tried several things to get even more specific details about the probem, after we found the specifig point of failure.

\begin{itemize}
    \item We have built a new try-catch around it - just in case. But the catch is not called because no exception is thrown.
    \item Because we use Framework and not Core, we could not debug step by step on macOS.
    \item Our manuel debugging with logging informations was not helpful since logging was not executed after the server start try.
    \item We also tried to start a very simple Language Server - without the Dafny project - and could not execute it correctly with \code{msbuild}.
    \item We tried diffrent mono versions: 6.6.0.166 (2019-08) and 6.8.0.105 (2020-02).
    \item We tested on macOS as well on Linux.
\end{itemize}

This made troubleshooting very difficult for us.
Unfortunately, we could not find out why the OmniSharp server startup does not work correctly with mono.
Unfortunately the OmniSharp community could not help us either \cite{mono-slack}.\\

While researching GitHub issues, it turned out that there are fundamental problems with Linux and Mac, as OmniSharp developers primarily develop and test on windows \cite{mono-git}.

In consultation with our supervisor, we then put the problem on hold after a certain amount of time had been used up.
Later on, Fabian Hauser noticed that instead of \code{msbuild} we would have to switch to \code{dotnet build}.

Since \code{msbuild} is required for our Sonar scanning and the limited time left,
we decided not to take up the problem again in the last weeks. \\

However, this would certainly be a good approach to solving the problem.
Furthermore it should be possible to switch the actual Dafny language server back to .Net Core
and leave only the Dafny core part on .Net Framework.
Apart from the Dafny component no .Net Framework features are used.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Testing}

This section provides a general overview of the testing.
It is split into unit, integration and system tests.
To read how to write tests or why we worked with interfaces for dependency injection, refer to the development document.

\subsubsection{Unit Tests}
Our core logic components all use an interface.
When using components, programming is done exclusively against the corresponding interface. \\

This encapsulation of components allows essential core logic to be efficiently covered by unit tests. \\

Due to the fact that our unit tests are run significantly faster than the integration tests, it is very important that unit tests are written.
Due to the fast run-through they are a very good support in the ongoing development.
If an error creeps in due to a change, it will be detected immediately by the existing tests.
If an error needs to be corrected or a function needs to be extended, a Test-Driven-Development approach can be used to work in a goal-oriented manner. \\

In our unit tests, primarily isolated and more complex logic blocks are tested.
These are not only covered by a Code Coverage metric, as described in the chapter "Project Management"
\ref{section:project_management} \nameref{section:project_management},
but we were also busy covering edge cases by testing.

Of course the interaction of the components is also tested.
This will be discussed in the following section regarding integration tests.

\subsubsection{Integration Tests}
As described in section \ref{chapter:designTests}, a very nice test architecture was builded for integration tests.
Each feature could be tested by creating a base class.
The base class usually contains one method \code{Run} and another one \code{Verify}.
The first one uses the inherited client-server infrastructure, opens a Dafny file, sends the according request, and collects the results.
The following example is representative for such a method:

\begin{lstlisting}[language=csharp, caption={Finding a Declaration}, captionpos=b, label={lst:visitorfinddecl}]
public void Run(string testfile, int lineInEditor, int colInEditor, string newText = "newText")
{
    Client.TextDocument.DidOpen(testfile, "dfy");
    RenameParams p = new RenameParams()
    {
        NewName = newText,
        Position = new Position(lineInEditor-1, colInEditor-1),
        TextDocument = new TextDocumentIdentifier(new Uri(testfile))
    };
    var response = Client.SendRequest<WorkspaceEdit>("textDocument/rename", p, CancellationSource.Token);
    result = response.Result;
}
\end{lstlisting}

The \code{Verify} method will just compare the results against the provided expectation.
Often, rather complex data structures with a lot of nested classes come into play.
To be able to compare them easily, most of them are just converted to a string representation, which can easily be dealt with.
This was done using extension methods located in the \code{TestCommons} project.\\

A test itself is created very easily with all this infrastructure.
One simply inherits from the base class, and most tests can be written in just a few lines.
For example, a rename test could look like this:

\begin{lstlisting}[language=csharp, caption={Sample Integration Test}, captionpos=b, label={lst:sampleintegrationtest}]
[Test]
public void LocalVariableUsage()
{
    Run(Files.rn_scope, 9, 17);
    List<string> expected = new List<string>()
    {
        "newText at L7:C12 - L7:C15",
        "newText at L8:C14 - L8:C17",
        "newText at L12:C18 - L12:C21",
        "newText at L20:C14 - L20:C17"
    };
    Verify(expected);
}
\end{lstlisting}

Note that the test is kept as concise as possible.
The tester does not even have to care about the result provided by \code{Run}, since that method will store the result inside a class member and clean it after the test is done.\\

Since all tests base on actual Dafny files, a dedicated subfolder was created to store them.
All files can be referenced globally from within the \code{TestCommons} project, where also the base class for all integration tests is located.



 \intnote{todo (?)}
\textbf{m}
\textbf{m}
\textbf{m}
\textbf{m}
\textbf{m}
\textbf{m}


\subsection{Usability Test}
Since we have been developing and using our plugin for many months,
it is sometimes difficult to differentiate the results.

Therefore we have designed a usability test and found a test person to perform this test \cite{interview-remo}.
The test person, Remo Herzog, was chosen because he is programming in an environment based on VS Code
and therefore is well integrated into the corresponding VS Code universe. \\

The goal of this test was primarily to find out if the features
we implemented were implemented as a VS Code Plugin user would expected them to be integrated into the VS Code IDE,
and which features were expected to be different in terms of display and triggering. \\

The general usefulness would also be discussed and feedback for further improvements was collected.
These improvements are discussed in this chapter.

\subsubsection{Code Documentation for Classes and Methods}
If you drive over symbols and the hover information is displayed,
and if the auto completion is listing symbols,
a code documentation is missing in the corresponding info boxes.
It would be desirable to have a code documentation for methods and classes,
which is displayed additionally with the mentioned features. \\

This feature could be realized with the new symbol table. The approach would be the following:

Through the feature CodeLens all declarations of methods and classes are already known.
For each of these symbols the correct position in the Dafny source code could be read out
and with a regular expression found code documentation comments could be read out.
The documentation text, read out by the pattern matching, can then be stored in the corresponding symbol as code documentation.
This information can then be sent to the client via the LSP and displayed,
for example in the case of auto completion or hover information,
as a \code{documentation} element \cite{vscodeAPI}.

\subsubsection{Cleaner Hover Information}
The information which is displayed in a symbol during a hover event was felt to be somewhat confusing.
Sometimes the terms like \code{kind} were not clear.
At best, some text would be appropriate.
Texts could be minimized by a better grouping.
Less text with the same information content.\\

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{tmpPlaceHolderTodo}
    \caption{Hover Before (left) and Cleaner Version (right)}
    \label{fig:placeholder_ref}
\end{figure}

\ref{fig:placeholder_ref}
Durch das erhaltene Feedback, wurde der Hover klarer strutkuriert, wie in Abbildung xxx gezeigt....
 \intnote{Todo unrelevantes löschen, positionen in llammern setzen }

\subsubsection{Automatic Triggering of Auto Completion}
If a dot is typed while typing, auto completion suggestions are automatically expected.
You should not have to press \code{control + space} as trigger.

In addition, the suggested methods lacked to inform about the supported parameters.
With regard to the parameters, this has already been discussed in section  \intnote{Todo}. \\

\intnote{... todo das sollte hoffentlich noch machbar sein mit dem lsp trigger character....
wenigstens etwas noch einbauen oO}
Durch blablabla konnte ein automatisches triggern der autocompletion nach einem dot aktiviert werden.

\subsubsection{Error Message Without Brackets}
todo iwie mit den eckigen klammern hat verwirrt. dann kommt teils ({) statt {
das evt reduzieren oder eleganter lösen...

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{tmpPlaceHolderTodo}
    \caption{Error Message before (top) and after (bottom)}
    \label{fig:cleaner_error_msg}
\end{figure}

Cleaner...:
\ref{fig:cleaner_error_msg}

\subsubsection{Compile Improvements}
Some plugins have a small green play icon in the upper right corner of the IDE for compilation,
which can be pressed. This is not offered by TypeScript, but by the Python support plugin for VS Code \cite{interview-remo}.
This play button has been missed during the usability test. \\

We could also add this icon for Dafny. If the button is clicked, a compile and run is triggered.
Similar to the UI elements for the Dafny statusbar,
a button can be created by the VS Code API \cite{vscodeAPI}.
It can be assigned an icon and a click action. \\

When running the Dafny program,
a lot of text messages are displayed,
which distract from the relevant output.
So the absolute path to the process and then the absolute path to the exe is displayed.

 Since, as shown in figure \ref{fig:long_compile_output}, the paths are mostly identical,
 it would make sense to reduce the output by specifying relative paths for the exe to be executed.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{tmpPlaceHolderTodo}
    \caption{Output to be Improved}
    \label{fig:long_compile_output}
\end{figure}

\subsection{Continuous Integration (CI)}
To optimize the CI process is an important part of our thesis.
Since we expect that our language server as well as the Dafny client plugin will be further developed by other developers,
the CI is a primary component of automated quality assurance. \\

This includes that the CI pipeline will fail if certain quality attributes are not met.
These include, in particular, successful completion of automated tests,
static code analysis and formatting control of the TypeScript code. \\

More details on the quality aspects are described in chapter \ref{section:project_management} \nameref{section:project_management}.
This section describes how the planned improvements for the CI process from chapter \ref{section:analysis:CI} were implemented.

\subsubsection{SonarQube}
According to our research, a major problem was that the scanner for SonarQube can only analyze one language at a time \cite{sonar-supports-only-one-language}.
This means, that the TypeScript code in the client and the \CsharpWithSpace code in the server cannot be analyzed simultaneously.
Furthermore, in the pre-existing Dafny project, single Java files appear, too.
This led to further conflicts in the Sonar analysis \cite{sa}.\\

As a simple solution, we decided to separate the client (VSCode plugin) and server (Dafny Language Server) into two separate git repositories.
This not only simplifies the CI process but also ensures a generally better and clearer separation. \\

As a result, the client could still be easily analyzed with the previous Sonar scanner.
Regarding the server, a special Sonar scanner for MSBuild had to be installed, which publishes the analysis in a dedicated SonarCloud project \cite{dev}.
The available statistics are very helpful for code reviews.\\

As a little extra, not only our Dafny language server part is analyzed by Sonar,
but the whole Dafny project.
From the bugs, vulnerabillities, code duplications and code smells revealed
by this analysis, the whole GitHub team working on the Dafny project can benefit. \\

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth-2cm]{sonarexamaple.png}
    \caption{Example of a Useful Sonar Finding}
    \label{fig:sonarexample}
\end{figure}

Unfortunately, the code coverage by tests is not analyzed.
Searching for an alternative, \textit{OpenCover} was found as a very common tool for code coverage analysis in \Csharp.
Unfortunately, it only runs under Windows  \cite{opencover}.
The CI server bases on Linux, though.
During our research we came across \textit{monocov} \cite{monocov}.
This tool would run under Linux and analyze .NET Framework projects.
Unfortunately, this project was archived and has not been maintained for almost 10 years \cite{monocov}.

Since we would not gain much value with sonar code coverage, we decided not to pursue this approach any further.
The coverage information is provided by the ReSharper extension \textit{dotCover} \cite{dotcover} to the developers.

\subsubsection{Client End-to-End Tests}
The end-to-end tests base on a lot of dependencies, such as a headless instance of Visual Studio Code.
In consultation with our supervisors, we have removed these tests from the client project and replaced them with own specially written integration tests on the server side.
This can be justified with the client only containing a minimal amount of logic that is required for the visual representation.
Any other logic was moved to the server.

\subsubsection{Static Program Analysis and Formatting for TypeScript}
During development it became apparent that it would be useful to check the correct formatting
for the TypeScript code from the client in the CI.
Various tools are used locally, which automatically format the syntax correctly when saved.
These tools are described in the developer documentation \cite{dev}.
However, if someone does not install these tools,
and the code is not formatted according to the definition,
the CI pipeline will fail. \\

Prettier was selected for this function.
Alternatively, ESLint would have been an option \cite{eslint}.
This would also have automatically integrated a static code analysis in the CI.
However, we decided to use a combination of SonarLint and Prettier,
so that the local analysis tool is matched to SonarQube \cite{dev}.

\subsubsection{Docker}
For an easier testability of the CI, we installed Docker locally.
This allowed us to resolve CI issues locally and platform-independently (through the Docker Client) in case of problems.
More details are stated in the developer documentation \cite{dev}.
The documentation found there helps future developers to easily and efficiently test changes to the docker container locally. \\

%%%%%%%%%%%%%AUS 05 rüber genommen, noch toodo zum mergen:
\subsection{Integration Tests TODO MERGEN mit oben.}
 \intnote{corbat: kommt in die implementation. vollständig. da haben wir schon ein kapitel; mergen.}

\label{chapter:designTests}
Unlike in the preceding semester thesis, integration tests could be implemented using Omnisharp's language server client \cite{omnisharpClient}.
Each test starts a language server and a language client, then they connect to each other.
Now, the client can send supported requests, for example "get me the counter examples for file ../test.dfy".
The result can be directly parsed into our \code{CounterExampleResults} data structure and be compared to the expectation.
Thus, tests can be written easily and are very meaningful and highly relevant.

\subsubsection{Dafny Test Files}
Integration Tests usually run directly on \code{dfy} source files.
Those test files need to be referenced from within the test.
To keep the references organized, a dedicated project \code{TestCommons} was created.
Each test project has access to these common items.
Every test file is provided as a static variable and can thus be easily referenced.

\begin{lstlisting}[language=csharp, caption={Test File Reference}, captionpos=b, label={lst:semiExpectedCodeThing}]
public static readonly string cp_semiexpected = CreateTestfilePath("compile/semi_expected_error.dfy");
\end{lstlisting}
The class providing these references will also check, if the test file actually exists, so that \code{FileNotFoundErrors} can be excluded.

\subsubsection{String Converters}
Many tests return results in complex data structures, such as \code{CounterExampleResults}.
Comparing these against an expectation is not suitable, since many fields and lists had to be compared to each other.\\
To be able to easily compare the results against an expectation, a converter was written to translate the complex data structure into a simple list of strings.
For example, each counter example will be converted into a unique string, containing all information about the counter example.
All counter examples together are assembled within a list of strings.
This way, they can be easily compared against each other.\\
Since not only counter examples, but also other data structures such as \code{Diagnostic} were converted into lists of strings, the converters were held generic as far as possible.
The following listing shows how this was realized.
The method takes a enumerable of type T as an argument, and a converter which converts type T into a string.
Each item in the enumerable is then selected in the converted variant.

\begin{lstlisting}[language=csharp, caption={Generic Method to Convert an IEnumerable}, captionpos=b, label={lst:genericconverter}]
private static List<string> GenericToStringList<T>(this IEnumerable<T> source, Func<T, string> converter)
{
    return source?.Select(converter).ToList();
}
\end{lstlisting}

Calling the above method for counter examples are made as follows.
A list of counter examples is handed as the argument, and a \code{Func<CounterExample, string> ToCustomString} is handed as the converter.
The converter is also shown in the following code segment.
Not that it is defined as an extension method.

\begin{lstlisting}[language=csharp, caption={Converting CounterExamples to strings}, captionpos=b, label={lst:converterCEToString}]
public static List<string> ToStringList(this List<CounterExample> source)
{
    return GenericToStringList(source, ToCustomString);
}

public static string ToCustomString(this CounterExample ce)
{
    if (ce == null)
    {
        return null;
    }
    string result = $"L{ce.Line} C{ce.Col}: ";
    result = ce.Variables.Aggregate(result, (current, kvp) => current + $"{kvp.Key} = {kvp.Value}; ");
    return result;
}
\end{lstlisting}

Comparison of the results and the expectation is now very simple.
The expectation can just be written by hand as follows:

\begin{lstlisting}[language=csharp, caption={Expectation}, captionpos=b, label={lst:testexpectation}]
List<string> expecation = new List<string>()
{
    "L3 C19: in1 = 2446; ",
    "L9 C19: in2 = 891; "
};
\end{lstlisting}

The results can be converted to a string list using the defined \code{results.ToStringList()} method.
By taking advantage of the method \code{CollectionAssert.AreEquivalent(expectation, actual)} from nUnit's test framework, the two lists can be easily compared against each other \cite{nunitCollectionAssert}.

\subsubsection{Test Architecture}
Since every integration test starts the client and the server at first, as well as disposes them at the end, this functionality could be well extracted into a separate base class.
This class is called \code{IntegrationTestBase} and just contains two methods, \code{Setup} and \code{Teardown}.
These methods could be directly annotated with the proper nUnit tags, so that every test will at first setup the client-server infrastructure, and tear it down after the test has been completed.\\
It was considered if the \code{IntegrationTestBase} class should directly contain a class member\linebreak \code{T TestResults} to store the test results, as well as a method \code{SendRequest} and \code{VerifyResults}. While storing the test results could have been realized, this was not possible for the methods \code{SendRequest} and \code{VerifyResults}. The problem is, that these methods have different signatures from test case to test case. A compilation requests has differnt parameters (such as compilation arguments), than a goto-defintion request (which as a position as a parameter).\\
Instead, it was decided to create a second base class for each test case.
For testing compilation, this class is named \code{CompileBase} as an example.
It inherits from the \code{IntegrationTestBase} class and provides the member \code{CompilerResults}, as well as two methods \code{RunCompilaton(string file, string[] args)} and \code{VerifyResults(string expectation)}. One can now easily see the dedicated paramter list.\\
The test class itself inherits from its case-specific base class.
The tests itself are very simple.
For example, if we want to test if the compiler reports a missing semicolon, we could create a test class \code{public class SyntaxErrors : CompileBase}.
Note that we inherit from hour case-specific base class.
Thus, the methods \code{RunCompilation} and\code{Verify} are at our disposal.
That means, that hour test is as simple as follows:

\begin{lstlisting}[language=csharp, caption={Sample Test for Missing Semicolon}, captionpos=b, label={lst:demoTest}]
[Test]
public void FailureSyntaxErrorSemiExpected()
{
    RunCompilation(Files.cp_semiexpected);
    VerifyResults("Semicolon expected in line 7.");
}
\end{lstlisting}


As you can see, the test contains only two lines of code.
The first is stating the test file, the second one the test result expectation.
 y the way, the boolean values represent if there were errors and if an executable was generated.\\
The same applies for test about counter examples, GoToDefinition and other use cases.
Thus, the integration test architecture could be created in a way so that the creation of tests is extremely simple and user friendly.
The code can be kept very clean and contains no duplicated code.
Tests can easily be organized into classes \textendash{} considering compilation this could for example be the separation into logical errors, syntax errors, wrong file types and such.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth/2]{integrationTestDesign}
    \caption{Test Architecture on the Basis of Compilation}
    \label{fig:testArchitecture}
\end{figure}

 \intnote{todo ich finde solche genauen zhalnen und angaben zur test maschine gehören nicht zum design
sondern eher implementation, nicht? sprich testing der implementation .
"For reproducibility, nUnits randomizer was used"
- wurde benutzt. dh ist ja nicht mehr planung sondern realisierung}


\subsubsection{Performance Measurement}
To measure performance, a little algorithm was written that creates a pseudorandom Dafny file.
Within a class method, 10'000 LOC are generated, either containing
\begin{itemize}
    \item a variable definition: \code{var v142 := v16;}
    \item a variable access: \code{print v142;}
    \item a block scope: \code{while (true) \{ \dots}
    \item ending a blockscope: \code{\}}
\end{itemize}
The chance to create a variable or to create print statement is 90\%, thus the generated files will contain about 9'000 variable name segments, that have to be resolved.
This challenges the symbol table quite a bit.
Since the \code{textDocuemt/didOpen} notification is not awaitable, a random LSP request was sent after the opening to wait until all actions are finished.
Since we implemented compile in such a way, that it uses the precompiled \code{dafnyProgram}, we have chosen to run a compile command.
The test has shown, that the process completes within 20 seconds on a 3.4GHz machine, which seems quite reasonable for such a large file.

For reproducibility, nUnits randomizer was used, which will create the same 'random' test file on every run.
Technically, the file generation could be excluded from the test engine, but it was left inside, in case someone wants to create additional or other tests.

