\section{Implementation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Client}
Das Refactoring bereits hier mitmachen?
Dann gleich den "ISt zustand" mit dem Refactroing vom Design hier einflieessen lassen.
ISt für den Leser meine ich weniger verwirrend.


- download:
GitLab Pages (Fabian gm Meeting... hust erwaehnen)
Interfaces geschrieben, versucht API zu nehmen, ging ned also server
dank interface austauschbar
interface altem tool nachempfunden. teils namen passender gewaehlt.

- vs kapselung
einfache migration auf andere IDE ermoeglicht

config
man kann alles konfigurieren. support fuer dark mode.
sonstige config und so sachen wurden zentralisiert. gm code review.

\subsubsection{Veröffentlichung}


beschreiben interface download, wie das ausgetaucht werden kann.
bin hier nicht draus gekommen genau.






\subsubsection{Client (Code Review)}
After a joint code review together with our advisors, individual optimisation potential was identified.
This subchapter describes the associated improvements to the architecture. \\

Although interfaces were used for the individualized types,
the individual core components did not use their own interfaces.
To reduce coupling, isolated modules were formed in a comprehensive refactoring process.
The modules now no longer program on the class implementations, but against the interface. \\

For this purpose one importable module with the name \code{\_<Directory>Modules} was created for each directory.
Figure \ref{fig:client_2nd_refactoring} shows an overview of the interfaces.
In addition, the dependencies among each other are shown.
For simplicity, the contents of \code{stringRessources} and \code{typeInterfaces} have been omitted. \\

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{client_2nd_refactoring.png}
    \caption{Second Major Client Refactoring}
    \label{fig:client_2nd_refactoring}
\end{figure}

At first glance, the architecture appears much tidier.
The dependencies are now pointing from top to bottom.
Methods have been simplified and the number of parameters could be reduced significantly.
Component identifiers have been renamed to be more understandable. \\

However, it is now also noticeable that there are considerably more dependencies on \code{stringRessources}.
While in the previous version only the module \code{ui} used \code{stringResources}, it is now used by almost all other modules.

This has the following reason: Up until this refactoring, the task of \code{stringResources} was to be a central collection of all UI strings. \glsadd{UI}
In the code review, it was decided that default values should no longer be set within the independent modules,
but rather at a central location.
This would make it easier to maintain these values. \\


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Server}

todo roadmap kapitel überblick einleitung

\subsubsection{Server Launch}
todo "corbat-platsch"
As done in common practice, the \code{Main} function is kept very short.
All it does is launching the language server, which is already handled by another class.

\begin{lstlisting}[language=csharp, caption={Main Function}, captionpos=b, label={lst:main}]
public static async Task Main(string[] args)
{
    DafnyLanguageServer languageServer = new DafnyLanguageServer(args);
    await languageServer.StartServer();
}
\end{lstlisting}

The launch of the server itself is divided into four stages.
First, preparational work is done.
This happens already in the constructor of the language server.
Preparation includes
\begin{itemize}
    \item Reading and processing config variables
    \item Setting up the logging framework
\end{itemize}
Secondly, the actual server is launched.
The logger will directly be injected and all handlers are registered.
In the third stage, once the server is running, a message sending service is instantiated to notify the client about the successful server start and, if any, errors occurred during start up.
Lastly, the console output stream is redirected to keep the language server stream isolated.

\begin{lstlisting}[language=csharp, caption={Starting the Language Server}, captionpos=b, label={lst:serverstart}]
public async Task StartServer()
{
    log.Debug(Resources.LoggingMessages.server_starting);
    server = await LanguageServer.From(options => \dots  );
    ExecutePostLaunchTasks();
    await RedirectStreamUntilServerExits();
    log.Debug(Resources.LoggingMessages.server_closed);
}
\end{lstlisting}

\subsubsection{Tools}
Within the tools package,
a variety of services can be found that do not necessarily directly correspond to Dafny,
but are useful within the language server environment.\\

\textbf{Config Initializer}\\
This class is used prior to the server launch and initializes a few config settings.
The settings are stored within the static class \code{Commons/LanguageServerconfig.cs}.
The config initializer will first of all set hard coded default values to avoid any kind of null pointer exceptions.
Afterwards, the file \code{Config/LanguageServerConfig.json} is parsed with Newtonsoft's Json.NET library \cite{jsondotnet}.
Any available values will be written to the static configuration class.
Unknown or illegal values will not be set and errors are added to an error reporter.
Finally, the launch arguments are parsed, again overwriting the config settings if applicable or reporting errors otherwise.
A simple argument parser was implemented manually.
Alternatively, a library could have been used for this task such as \cite{clparser}.
The config initializer is implemented exception safe.
This will run to completion and at worst just provide default values.
Errors can later be extracted from the \code{ErrorReporter}.
Each task is done by a dedicated component.\\

\textbf{LoggerCreator}\\
This class simply sets up a \textit{Serilog} \cite{serilog} logger.
For this purpose, the following information is extracted from the \code{LanguageServerConfig}:
\begin{itemize}
    \item Minimum loglevel
    \item Path of the logfile
\end{itemize}

\textbf{MessageSenderService}\\
This is a simple class accepting a \code{ILanguageServer} in the constructor.
Afterwards, it provides methods to send notifications to the client.
Similar to logging, methods for each severity level are available, such as \code{public void SendError(string msg)}.\\

\textbf{ReservedWordsProvider}\\
This is a class providing a set of words, that are not suited for identifiers.
This is, for example, 'method', 'class', or 'return'.
The class tries to read and parse \code{Config/ReservedDafnyWords.json}, which can be user adjusted in case the Dafny specification changes.
If the file cannot be read or has a wrong format, a hard coded default list is used which was taken out from the Dafny Reference Manual \cite{dafnyReferenceManual}.
A HashSet was used to provide fast access to the set.\\

While this component is specifically used solely for the \code{rename}-Feature, it was extracted to be also available at other spots if required for future features.

\subsubsection{Handler}
Handlers are passed to the language server and are called whenever the language server receives a corresponding request.
Services, such logging or workspace management, can be injected and are thus available for the handler.
OmniSharp directly defines interfaces, which have to be implemented in the handler classes.
For example, the interface \code{IDefinitionHandler} requires the class to implement a \code{Handle} method.
Every time the server receives a \code{textDocument/Definitions} request, this \code{Handle} method will be called.
The parameter and return types are specific per request.
Go to Definition would pass a text document location as input, namely the cursor position, and it expects a \code{LocationLink} as response, namely where the cursor should jump to.
Own requests can be realized according to chapter \ref{chapter:customlspmsg} by defining an own interface.\\

All handlers require two additional methods, aside the actual \code{Handle}:
\begin{itemize}
    \item GetRegistrationOptions: This method is called when the handler is registered.
    It allows to set a document selector.
    This is, that the handler is only active for '.dfy' files.
    \item SetCapability: It allows to set handler-specific capabilities, such as if the rename-handler will also support a 'prepare rename' feature.
\end{itemize}
The code for these methods is always identical and was thus extracted to a generic base class.
The generic type parameter refers to the kind of capability, necessary for the \code{SetCapability} method.
This could be, for example, \code{RenameCapability} which has the mentioned bool property about the preparation support.\\

To keep all this boilerplate code separated from the core logic, the actual \code{Handle} method will in most cases just create a provider instance, where all core logic is placed, and forward its result.
For example, the full code of handling a compilation request is shown below.

\begin{lstlisting}[language=csharp, caption={Handling Compilation}, captionpos=b, label={lst:handlecompilation}]
public async Task<CompilerResults> Handle(CompilerParams request, CancellationToken cancellationToken)
{
   _log.LogInformation(string.Format(Resources.LoggingMessages.request_handle, _method));
   try
   {
       FileRepository f = _workspaceManager.GetFileRepository(request.FileToCompile);
       return await Task.Run(() => f.Compile(request.CompilationArguments), cancellationToken);
   }
   catch (Exception e)
   {
       HandleError(string.Format(Resources.LoggingMessages.request_error, _method), e);
       return null;
   }
}
\end{lstlisting}

The other handlers look very similar to listing \ref{lst:handlecompilation}.
It requests the injected workspace manager for the file and passes it to the core logic provider.
Finally, the result is returned as a result.
In case of an error, a message is sent to the user and the error is logged within the \code{HandleError} method.\\

Some handlers take additional actions, such as awaiting the result of the provider, and then sending user feedback according to the outcome.
This way, the message sending service does not have to be passed downwards to the core logic component.

\subsubsection{Workspace}
The workspace is a component representing any opened files by the client.
Thus, it naturally consists only of a single property.
This is a dictionary, mapping a file-location to an internal file-representation:

\begin{lstlisting}[language=csharp, caption={Workspace Property}, captionpos=b, label={lst:workspaceproperty}]
private readonly ConcurrentDictionary<Uri, FileRepository> _files;
\end{lstlisting}

It offers methods to retrieve files and to update them.
Since updates can be down in two different kinds, incremental or full, the update method is overloaded for both cases.

More interesting is the class \code{FileRepository}, which is used as the internal representation of a file.
It of course contains the source code.
However, this is not done directly as a string property, but wrapped in a class \code{PhysicalFile}.
Thus, the actual representation on the hard disk is separated even further.
The \code{PhysicalFile} class can then also take responsibility for applying file updates.\\

\intnote{ich wrüde hier gerne ein bild einfügen aber das wäre das gleiche wie beim design.. grml.}.\\

Aside the file content, each \code{FileRepository} will also contain \code{TranslationResults}.
\code{TranslationResults} is a wrapper class for anything provided by the Dafny backend:
\begin{itemize}
    \item Could the file be parsed?
    \item Could it be verified?
    \item Is it logically correct?
    \item What errors and warnings occurred?
    \item How far could it be compiled?
    \item What internal compilation results could be produced for later reuse?
\end{itemize}

Last but not least, the newly implemented symbol table is also attached to the file repository.
Thus, all information about a file is accessible from within the file repository.
To obtain all of these results, the class simply invokes the \code{SymbolTableGenerator} and the \code{DafnyTranslationUnit}.

\begin{lstlisting}[language=csharp, caption={Handling Compilation}, captionpos=b, label={lst:handlecompilation}]
public interface FileRepository
{
    PhysicalFile PhysicalFile { get; }
    TranslationResult Result { get; }
    ISymbolTableManager SymbolTableManager { get; }
    void UpdateFile(string sourceCodeOfFile);
    void UpdateFile(Container<TextDocumentContentChangeEvent> changes);
}
\end{lstlisting}


\subsubsection{DafnyAccess}
Dafny Access it the package invoking the Dafny backend to obtain verification results.
The core class in this package is the \code{DafnyTranslationUnit}.
It was partially taken over from the pre-existing projects, but as part of this bachelor thesis it was refactored and simplified.\\

In the constructor, the translation unit accepts a \code{PhysicalFile}.
The class then offers a single public method \code{public TranslationResults Verify()}.
Within the method, the following sequence of events occurs:
\begin{enumerate}
    \item It is checked that the instance has never been used before.
    This is, since the error reporter must be empty.
    Otherwise, errors would be reported multiple times.
    \item Next, Dafny is configured.
    This includes the registration of the Dafny error reporter and setting any options to default.
    The only non-default option is that the engine is supposed to generate a model file, which can later be used for counter example calculation.
    The configuration is shown in \ref{lst:setupdafnyoptions}.
    \item The Dafny parser is called.
    This step will report any syntax errors.
    \item The Dafny Resolver is called, if parsing was successful.
    This step will do semantic checks, such as type checks.
    \item If successful, the precompiled \code{DafnyProgram} will be split into \code{BoogieProgram}s.
    \item The Boogie Execution Engine is invoked to perform logical correctness checks on the \code{BoogieProgram}s.
    \item Any errors that were reported are collected, converted and provided in the field \code{\_diagnosticElements}
    \item All results are wrapped by the \code{TranslationResult} class, providing the diagnostics, the Dafny program and the boogie programs.
    Also, within the property \code{TranslationStatus}, it is remarked how far the verification and translation process succeeded.
\end{enumerate}

\intnote{evtl bild, so ablaufdiagramm}



\begin{lstlisting}[language=csharp, caption={Setting up Dafny Options}, captionpos=b, label={lst:setupdafnyoptions}]
private void SetUpDafnyOptions()
{
    DafnyOptions.Install(new DafnyOptions(_reporter));
    DafnyOptions.Clo.ApplyDefaultOptions();
    DafnyOptions.O.ModelViewFile = FileAndFolderLocations.modelBVD;

}
\end{lstlisting}


\subsection{Symbol Table}
This package provides four components:

\intnote{bilder hier ist schwierig weil es einfach das gleiche ist wie bei design...)}\\

\begin{itemize}
    \item Symbol Information
    \item Symbol Table Generation
    \item Symbol Table Navigation
    \item Symbol Table Management
\end{itemize}

\textbf{Symbol Information}
todo uml
This is a component that summarizes all information about a symbol.
Aside the name, this also includes the location, the parent, the declaration, children, and so forth.
The class contains a lot of properties, but in exchange, it provides any information that is required.
The most important properties are:
\begin{itemize}
    \item Name
    \item File
    \item Position in File
    \item Body Size, if any
    \item Kind and Type
    \item Link to Parent Symbol
    \item Link to Declaration Symbol
    \item Hash with all Children Symbols (Only Declarations)
    \item List with all Descendants (Any symbol occurring in the body)
    \item List with all Usages of the symbol
    \item BaseClasses
    \item The associated module
    \item Link to the associated default class for quick access.
\end{itemize}
The provided properties are supposed to facilitate working with the symbol table.
For example, if a symbol's definition cannot be found with regular methods, the property with the associated default class can be accessed to search the symbol there.
No separate logic to find the default class is necessary.
This is advantageous, since the default class is in the global namespace and not a direct ancestor of a symbol.\\

Technically, the symbol table is more of a tree, then a table.
The data structure is double linked.
Each symbol knows about its descendants, but also about its ancestor.
Navigating to either one can thus be done in $O(1)$.
If the name of a descendant is known, navigating to the symbol can also be done in $O(1)$ due to the hash map.
For example, if a module \code{M} contains a class \code{C}, and within the class there is a method \code{foo}, one can simply start from the root symbol and navigate through the hash maps.
The \code{[]} operator was overloaded to make this as convenient as possible:\\
\code{rootSymbol["M"]["C"]["foo"]}.\\

To enable efficient access to the children of a symbol, we have opted for a key-value data structure.
The key is the child symbol's name, the value the actual \code{SymbolInformation} object.
This hash structure enables access to a child symbol with a runtime of $O(1)$.
Since every symbol also has a link to it's parent, navigation in both ways can be done within $O(1)$.\\

While this is very fancy, the convenience comes at a price.
Many properties do not apply for all kinds of symbols.
Consider the following code segment:

\begin{lstlisting}[language=dafny, caption={Example Code Regarding Symbol Information}, captionpos=b, label={lst:aldbkajds}]
method foo() {
    var x := 5;
    print x;
}
\end{lstlisting}

The symbol \code{foo} profits by almost all properties.
It can have children (the variable x), it has some parent, it can be used.
Since foo is a method declaration, the declaration property of the symbol does not make sense.
In this case, it actually just points to itself.
The declaration of \code{x} in the second line will have many null values within the symbol information.
While the parent is \code{foo}, it can not have any children.
Since it is a variable definition, it can have usages though.
The final usage of \code{x} in the last line (which we also consider a symbol), does not even have usages, since it is a usage itself.
Thus many properties are just null for that symbol.\\

As a consequence, operating on the symbol table must be done with possible null reference expectations in mind.
Null checks were added to the code wherever necessary.\\

Aside the properties, the \code{SymbolInformation} also offers a public method.
This method will check if a certain location is wrapped by the symbol.
This namely answers the question, if for example line 5, column 2 is within the symbol's body.\\

\textbf{Symbol Table Creation}\\
The symbol table generator accepts a precompiled Dafny program in the constructor.
The generator offers a public method \code{GenerateSymbolTable}.
It will first of all create a virtual root symbol.
Any other symbols will be attached to the root node as descendants.
The root node is also the final return value.\\

Then, all modules (similar to \CsharpWithSpace or C++ namespaces) will be extracted out of the Dafny program.
The modules will be sorted by depth, so that top level modules will be treated first and nested modules can be attached properly later on.\\

Once the modules are sorted by depth, the algorithm iterates over each of the modules.
The proper parent symbol will be extracted.
For a top level module, this is just the root symbol.
Otherwise, for nested modules, the parent module will be located.
Finally, the module will accept the declaration visitor.
The visitor is described later.
Once it has completed, all symbol declarations are registered in the symbol table.
A second iteration is then started, using a visitor, which will ignore declarations but run through all method bodies and take care of symbol usages.\\
This way, symbols that are declared after the first usage can be found as well.\\

\textbf{Visitor}\\
As already mentioned, the whole symbol table generation is realized using the visitor pattern.
For that, Dafny code had to be adjusted to offer a \code{Accept(Visitor v)} method.
This method will basically just navigate through the internal Dafny symbol representation.
For example, when visiting a method, one would like to register the method itself.
This is done by the expression \code{v.Visit(this)}.
However, the method also contains a Dafny \code{ensures} statement, which may contain further variables.
Thus, all statements within the \code{ensures} clause have to be visited.
The \code{Accept}-method will now just forward the call, using \code{foreach (var e in this.EnsureStatements) {e.Accept(v)}}.
The same applies for method parameters and other items like the requires clause.
Finally, the body of the method is to visit using \code{foreach (var stmt in this.Body) {stmt.Accept(v)}}.
Once everything is done, the scope of the method is left by calling \code{v.Leave(this)}.\\

If you recall the last section, it was said that two runs are performed.
One to capture all declarations, and one to visit all method bodies.
Thus, the visitor is having a boolean property \code{GoesDeep}, which decides if method bodies are visited or not.
The final \code{Accept} method for a method looks as shown below.
The method is shortened, there are more clauses like for example the requires clause.


\begin{lstlisting}[language=csharp, caption={Accepting a Visitor}, captionpos=b, label={lst:visitoraccept}]
public override void Accept(Visitor v)
{
  v.Visit(this);
  if (v.GoesDeep)
  {
    foreach (var ens in this.Ens)
    {
      ens.Accept(v);
    }
    foreach (var stmt in this.Body.Body)
    {
      stmt.Accept(v);
    }
  }
  v.Leave(this);
}
\end{lstlisting}

Note that the method is marked with the override keyword.
This is, since every AST-Element is either a statement or an expression, among others.
In case a specific AST element is not treated separately, a general accept method is defined for statements, as well as expressions.
However, for supported AST elements, a more specific method is supposed to be used.\\

On the other hand of the acceptor, there is the actual visitor.
The visitor has to implement \code{Visit} methods for each of the AST elements that it is supposed to visit, for example our \code{Method} from the preceding example.\\

The Visitor itself will now actually build up the symbol table.
For that, it stores the current scope in a property.
For example, when visiting a method, the parent scope is always some kind of class that was visited before.
The visitor will now create a symbol for the method, and the property 'parent' can just be set with the scope the visitor has stored - the class mentioned before.
Since the method itself will have it's own body, the new scope can then be set to the method.
All symbols that will be visited afterwards - which is anything inside the method - are then attached to the method.
Once the method is done, \code{Leave()} is called, which will reset the scope to the parent class for the next item to be visited.\\

The \code{Visit} method of the first visitor, which only takes care of declarations, will just create a symbol, and attach it to the proper scope.
All required information is taken from the AST element that is visited.
This includes the name, the position, and so on.\\

\begin{lstlisting}[language=csharp, caption={Visiting a Method}, captionpos=b, label={lst:visitorvisit1}]
public override void Visit(Method o)
{
    var symbol = CreateSymbol(
        name: o.Name,
        kind: Kind.Method,

        positionAsToken: o.tok,
        bodyStartPosAsToken: o.BodyStartTok,
        bodyEndPosAsToken: o.BodyEndTok,

        isDeclaration: true,
        declarationSymbol: null,
        addUsageAtDeclaration: false,

        canHaveChildren: true,
        canBeUsed: true
    );
    SetScope(symbol);
}
\end{lstlisting}

The \code{CreateSymbol} method will set all properties accordingly.
That means, a symbol that can have children will be initialized with a list for children, while a symbol that cannot have children will just have a null entry there.
Note that the scope of the visitor is set to the method for future visitations.\\

The second visitor will also visit declarations, but no longer create a symbol for them.
Instead, the already created symbol is located and set as the proper scope.
Once a method body is encountered, it will now also be visited.
Within the body, local variables exist.
Despite local variables being declarations, they were not handled by the first visitor, which is actually responsible for declarations.
However, this is fine, since local variables are not accessible before they were not declared.
Furthermore, symbol usages such as method calls or variable usages are now encountered.
The visitor has to create proper symbols for these.
Since these are symbol usages, it is not sufficient to just create a symbol and attach it to the parent scope.
The following additional tasks have to be done:
\begin{itemize}
    \item Where is the symbol declared?
    \item Add usage to the symbol's declaration
\end{itemize}

To find the declaration, the symbol table navigator can already be used.
The navigator will just iterate from parent to parent and will return the first symbol, that is a declaration and matches the name.
Challenges occurred when a symbol is defined in global scope or in a inherited base class.
Both difficulties were resolved by adding separate checks for them.
Once the declaration is found, it is simple to add the just newly created symbol as one of the declaration's usages.\\

The main challenge of the whole visitor implementation was to find a symbols declaration.
The task could be extracted to the navigator:

\begin{lstlisting}[language=csharp, caption={Finding a Declaration}, captionpos=b, label={lst:visitorfinddecl}]
protected ISymbol FindDeclaration(string target, ISymbol scope, Kind kind)
{
    INavigator navigator = new SymbolTableNavigator();
    bool filter(ISymbol s) => s.Name == target && s.IsDeclaration && s.Kind == kind;
    return navigator.BottomUpFirst(scope, filter);
}
\end{lstlisting}

This snippet assigns the navigator to move upwards to tree, starting from the scope the visitor is just operating in.
In every scope, the navigator will iterate over all symbols and search one, that is a declaration and matches in name and kind.
For example, if we start inside a method, every symbol already visited in that method will be checked.
If no matching declaration was found, the navigator will move to the parent of the method, which must be a class.
Then, all symbols within that class are searched.
Now, the navigator may find a class method that actually matches the search criteria.
Since all declarations were registered beforehand, the process works even if the symbol is declared after the usage, unless it is a local variable, where it must be declared anyway before usage.\\

If the symbol cannot be found, the algorithm will search the default scope at the end as a last option to locate the symbol.
Since the symbol tree is moved upwards, the process takes $O(logn)$ time.
It is executed for every occurring symbol, thus the building of the symbol table takes $O(nlogn)$ time, while n denoted the amount of occurring symbols.\\

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{implementation/findDeclaration.png}
    \caption{Finding a Declaration using the Navigator}
    \label{fig:findDecl}
\end{figure}


\textbf{Symbol Table Navigator}\\
To operate on the (partially) constructed symbol table, a separate component to navigate was created.
It has basically two procedures.
Remember that the data structure of the symbol table is basically a double linked tree.

\begin{itemize}
\item TopDown: Starting from a node, the navigator dives downwards and searches a specific symbol.
\item BottomUp: Starting from a node, the navigator moves upwards the tree and searches a specific symbol, as seen in the previous chapter.
\end{itemize}

\intnote{2do Bild von nem baum und dann wie es so hoch und runter geht, Beispiel, Visualisierung einbauen fuer die Laufzeitanalyse.} \\

Both options are implemented so that they can return a single, first match, or any symbols that match a criterion.
To illustrate this, two examples are considered:\\

\textbf{What symbol is at the current cursor position?}\\
\code{TopDown} is called.
The rootSymbol and the current cursor position are handed as arguments.
The rootSymbol has 3 modules attached to it.
One ranging from line 1 to 20, another ranging from line 21 to 40.
The algorithm will now decide, in which of the two modules a further search is worth wile.
If the cursor is located at line 25, it will continue to search in the second module.
This can simply be done by calling the same function recursively, handing the second module as the entry point for the recursive search.
Within the recursive call, the proper class will be found, and so on.
Default namespaces and default classes had to be treated separately for this case.
Since the tree is moved along a single branch, the runtime for this process is $O(logn)$.\\

\textbf{What auto completion suggestions are indicated?}\\
To build up autocompletion suggestions, all declarations available at a certain locations must be figured.
First of all, the previous \code{TopDown} algorithm is used to find the current location.
Then, all declarations in the current scope are requested.
Afterwards, the algorithm moves to the parent scope, and will again search for all declarations.
This process is repeated until the root scope is reached.
Again, the problem can be solved using recursion.
This algorithm is also visiting just a single tree branch, thus requiring $O(logn)$ time.\\

The navigator offers further methods to visit the whole tree if that is desired by the caller.
The method \code{TopDownAll} will visit all branches, not just the one wrapping a certain location.
Consequently, \code{TopDownAll} requires $O(n)$ runtime and should not be used if possible.\\
\intnote{wozu brauchen wir die eig? war ursprünglich der modul wrapper}




\textbf{Symbol Table Manager}\\
The manager is a rather simple component and can be used as an access point to perform operations on the symbol table.
It is constructed with a root symbol of a fully generated symbol table and contains a navigator. \intnote{todo es würd echt sinn machen den navigator als klassenprop des managers, falls das unterzubringen is und schnell gemach sti}
It then offers methods such as \code{ISymbol GetSymbolByPosition(Uri file, int line, int character)}.
That method will use the navigator to provide the user with the desired result.
Access to the rather complex navigator is encapsulated this way.\\


\subsubsection{Core}
In this package, the actual task of providing results for the server is done.
Often, not much code is necessary, since the symbol table provides all necessary information.
The classes in this package are called \textit{providers}.
A few of them are explained more in detail.\\

\textbf{Go to Definition}\\
Go to definition is a very simple feature once the symbol table is created.
As parameters, the provider receives a location inside a file.
All it has to do is request what symbol is at this position from the \code{SymbolTableManger}.
This will do a top-down search and is done in $O(logn)$ time.
Since every symbol has a property, containing its declaration, the declaration can be accessed directly and renders as result of the task.
Finally, the provider converts the declaration position to the proper response format and returns it.\\

To provide a better usability, it does a few further checks, such as checking if the symbol already was a definition or if no result could be found at all.
The results are stored in an \code{Outcome} property, which the handler can use to send proper client feedback.\\

\begin{lstlisting}[language=csharp, caption={Providing Goto Definition}, captionpos=b, label={lst:gotoCore}]
public LocationOrLocationLinks GetDefinitionLocation(Uri uri, int line, int col)
{
   List<LocationOrLocationLink> links = new List<LocationOrLocationLink>();
   var symbol = _manager.GetSymbolByPosition(uri, line, col);
   if (symbol == null)
   {
       Outcome = DefinitionsOutcome.NotFound;
       return new LocationOrLocationLinks();
   }
   if (symbol.IsDeclaration)
   {
       Outcome = DefinitionsOutcome.WasAlreadyDefintion;
   }
   var originSymbol = symbol.DeclarationOrigin;
   Position position = new Position((long)originSymbol.Line - 1, (long)originSymbol.ColumnStart - 1);
   Range range = new Range { Start = position, End = position };
   var location = new Location { Uri = originSymbol.FileUri, Range = range };
   links.Add(new LocationOrLocationLink(location));
   Outcome = DefinitionsOutcome.Success;
   return new LocationOrLocationLinks(links);
}
\end{lstlisting}

\textbf{DiagnosticProvider}\\
This component is invoked every time a document is updated.
It accepts a \code{FileRepository} as an argument.
Within the repository, the translation results are stored, including the diagnostics.
This component will read the diagnostic, convert them to an LSP-suitable format, add usability information, and finally send the result back to the client.
Since this is just a conversion of already calculated \code{TranslationResults} by the \code{DafnyTranslationUnit}, the runtime of this provider is neglectable.\\

\textbf{HoverProvider}\\
This component is very simple.
It requests the \code{SymbolTableManager} to provide the symbol at the hover location - again this will require $O(logn)$ time.
The hover location is passed as an argument within the request.
Afterwards, basic information about the symbol is assembled and returned.
That information includes
\begin{itemize}
    \item A quick summary, including the name and the location
    \item Symbol kind
    \item Symbol type
    \item Parent symbol
    \item Declaration origin
\end{itemize}
and is instantly available, not adding any runtime penalties.\\

\textbf{RenameProvider}
Rename is another feature that profits strongly by the symbol table.
Again the feature requests the symbol at the cursor in $O(logn)$.
Afterwards, it jumps to it's declaration in $O(1)$.
The declaration has all usages of the symbol stored, and thus, all occurrences are also instantly accessible.
The provider will now just assemble a \code{WorkspaceEdit} and return it.
The \code{WorkspaceEdit} contains the new name and as well all \code{Range}s, where the name has to be applied.\\

Additionally, the rename provider performs a few checks, if the new name is valid.
The checks are
\begin{itemize}
    \item Name must not start with and underscore
    \item Name must not be a reserved word, such as 'method'
    \item Name must not contain any other then alphanumerical dinger or underscores
\end{itemize}

The last check is more restrictive than it needs to be, but since special characters are extremely uncommon in programming, it is well suitable.
It was mainly implemented to prohibit brackets in names.\\

\textbf{Completion}\\
todo
Recht komplex
Wir supporten drei cases; nennen. Beschreiben.
Ablauf. Diagramme wie in der SA sind nützlich. das mag olaf bestimmt. \\

PAP oder flussdiagramm zeichnen für die fall untrscheidungen\\
das vom papier digitalisieren todo

\textbf{Code Lens}\\
todo
komplett server seitig nicht übers lsp möglich, weild er client die 'lens' aktion auslöst.
reicht daten für codelens popup an den client wieter; ruft vs code feature auf.
beide aspekte werden durch integration tests abgedeckt\\

\textbf{Compilation}\\
Every time a file gets updated, the whole Dafny backend is triggered and the results are stored in the \code{FileRepository}.
This includes the precompiled \code{dafnyProgram}.
The compilation provider will take advantage of that and use the precompiled item, and just forward it to the compiler engine.
Prior to the compilation, custom compilation arguments are installed, if the user provided any.
The process is fully integrated into the Dafny backend by using the following code line.\\

\begin{lstlisting}[language=csharp, caption={Calling the Dafny Compiler}, captionpos=b, label={lst:dafnycompiler}]
DafnyDriver.CompileDafnyProgram(dafnyProgarm, filePath, otherFiles, true, textwriter);
\end{lstlisting}

The provider will check if any errors occurred and return the outcome within a wrapper class.\\

\textbf{Counter Example}\\
This feature bases on the model file, which is generated during verification.
The model file is a key-value store generated by Boogie.
It contains several states.
Each state tracks the content of variables during different stages of the proof.
Of interest is primarily the \textit{initial} state, since this one contains the information how variables need to be set at the initially to achieve a counter example.\\

This provider reads the model file and uses the Boogie backend to convert into a useful format.
Afterwards, it extracts the initial state from the model.
Out of the remaining model, all key-value-pairs containing useful information are extracted, assembled and returned.
The component will also transform information into a more human readable format, e.g. $((- 12)) \rightarrow -12$.
Furthermore, many values are internal Boogie references and cannot be resolved.
These just look like \code{T@U!val!12}.
Such values are replaced with the text \code{[Object Reference]}.\\


\subsection{Mono Support for macOS and Linux}
\label{section:implementation:mono}

Eines der Kernzeiele war es, Support fuer mehrere Plattformen zu bieten. Dh nebst Windows auch macOS und Linux.
Da wir in unserer SA von Core auf Framework umsteigen musste, stand fest, dass wir mono fuer den Support auf Linux und macOS brauchen.
(warum in der SA; plficht wegen dafny core. was ist mono)

todo

Leider funktionierts nicht.
Anssaetze die wir probiert haben. verschiedene mono versionen, angefragt im slack. antwort erhalten?
github issues: allgemein probleme mit lunux/mac weil primaer auf windows und gar nicht auf mac getestet wird. (heikle aussage selbs tmit quelle)

ansatz in die implementation rein
ergebnis "wir habens probiert", die nächsten machen dass dann

- ms build wegen sonar, probleme in der SA mit core
dotnet build sollen wir umstellen...
unerer eig core fähig
danfy braucht bei der SA framework.. abhängigkeiten machten alles kaput, mit manuel nicht fixen können, framework genommen
inwzsichen ist diese dafny konsole / server weg weil wir das fancy gemacht haben
evt bestehen diese abhängigkeiten gar nicht mehr; umstellung auf core könnte leicht sein
mangelnde zeit daher nicht gemacht. als ausblick festhalten.

\cite{sa}
\cite{mono-slack}
\cite{mono-git}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Testing}

This chapter provides a general overview of the testing.
It is split into unit, integration and system tests.
To read how to write tests or why we worked with interfaces for dependency injection, refer to the development document.

\subsubsection{Unit Tests}
Our core logic components all use an interface.
When using components, programming is done exclusively against the corresponding interface. \\

This encapsulation of components allows essential core logic to be efficiently covered by unit tests. \\

Due to the fact that our unit tests are run significantly faster than the integration tests, it is very important that unit tests are written.
Due to the fast run-through they are a very good support in the ongoing development.
If an error creeps in due to a change, it will be detected immediately by the existing tests.
If an error needs to be corrected or a function needs to be extended, a Test-Driven-Development approach can be used to work in a goal-oriented manner. \\

In our unit tests, primarily isolated and more complex logic blocks are tested.
These are not only covered by a Code Coverage metric, as described in the chapter "Project Management" todo REFERENZ, but we were also busy covering edge cases by testing.

Of course the interaction of the components is also tested.
This will be discussed in the following chapter regarding integration tests.

\subsubsection{Integration Tests}
As described in chapter \ref{chapter:designTests}, a very nice test architecture was builded for integration tests.
Each feature could be tested by creating a base class.
The base class usually contains one method \code{Run} and another one \code{Verify}.
The first one uses the inherited client-server infrastructure, opens a Dafny file, sends the according request, and collects the results.
The following example is representative for such a method:

\begin{lstlisting}[language=csharp, caption={Finding a Declaration}, captionpos=b, label={lst:visitorfinddecl}]
public void Run(string testfile, int lineInEditor, int colInEditor, string newText = "newText")
{
    Client.TextDocument.DidOpen(testfile, "dfy");
    RenameParams p = new RenameParams()
    {
        NewName = newText,
        Position = new Position(lineInEditor-1, colInEditor-1),
        TextDocument = new TextDocumentIdentifier(new Uri(testfile))
    };
    var response = Client.SendRequest<WorkspaceEdit>("textDocument/rename", p, CancellationSource.Token);
    result = response.Result;
}
\end{lstlisting}

The \code{Verify} method will just compare the results against the provided expectation.
Often, rather complex data structures with a lot of nested classes come into play.
To be able to compare them easily, most of them are just converted to a string representation, which can easily be dealt with.
This was done using extension methods located in the \code{TestCommons} project.\\

A test itself is created very easily with all this infrastructure.
One simply inherits from the base class, and most tests can be written in just a few lines.
For example, a rename test could look like this:

\begin{lstlisting}[language=csharp, caption={Sample Integration Test}, captionpos=b, label={lst:sampleintegrationtest}]
[Test]
public void LocalVariableUsage()
{
    Run(Files.rn_scope, 9, 17);
    List<string> expected = new List<string>()
    {
        "newText at L7:C12 - L7:C15",
        "newText at L8:C14 - L8:C17",
        "newText at L12:C18 - L12:C21",
        "newText at L20:C14 - L20:C17"
    };
    Verify(expected);
}
\end{lstlisting}

Note that the test is kept as concise as possible.
The tester does not even have to care about the result provided by \code{Run}, since that method will store the result inside a class member and clean it after the test is done.\\

Since all tests base on actual Dafny files, a dedicated subfolder was created to store them.
All files can be referenced globally from within the \code{TestCommons} project, where also the base class for all integration tests is located.



todo (?)
\textbf{m}
\textbf{m}
\textbf{m}
\textbf{m}
\textbf{m}
\textbf{m}


\subsection{Usability Test}

todo

erkentnisse notieren. was machbar, war nicht. was haben wir noch geschafft. vorher nachher screen.
zb hover information fix machbar. button meint corbat ned nötig. evt link auf vs code dokumentation für button play.

Entgegen der ursprünglichen Annahme, nicht "alt und neu" als vergleich testen lassen (fehlende probanden)
Testing der allgemeinen Benutzbarkeit unserer Features (verständlichkeit, nützlichkeit)
Testing der VSCOde Universe Integration (VSCOde Benutzer)

Auswertung der Tests; Kernpunkte und Verbesserungsvorschlägather
Drauf eingehen, welches Feedback umgesetzt wurde, welches nicht, warum, outlooks.

Tests in den Anhang aufnehmen!! Und dann hier entsprechend referenzieren.

\subsection{Continuous Integration (CI)}
To optimize the CI process is an important part of our thesis.
Since we expect that our language server as well as the Dafny client plugin will be further developed by other developers,
the CI is a primary component of automated quality assurance. \\

This includes that the CI pipeline will fail if certain quality attributes are not met.
These include, in particular, successful completion of automated tests,
static code analysis and formatting control of the TypeScript code. \\

More details on the quality aspects are described in chapter \ref{section:project_management} \nameref{section:project_management}.
This chapter describes how the planned improvements for the CI process from chapter \ref{section:analysis:CI} were implemented.

\subsubsection{SonarQube}
According to our research, a major problem was that the scanner for SonarQube can only analyze one language at a time \cite{sonar-supports-only-one-language}.
This means, that the TypeScript code in the client and the \CsharpWithSpace code in the server cannot be analyzed simultaneously.
Furthermore, in the pre-existing Dafny project, single Java files appear, too.
This led to further conflicts in the Sonar analysis \cite{sa}.\\

As a simple solution, we decided to separate the client (VSCode plugin) and server (Dafny Language Server) into two separate git repositories.
This not only simplifies the CI process but also ensures a generally better and clearer separation. \\

As a result, the client could still be easily analyzed with the previous Sonar scanner.
Regarding the server, a special Sonar scanner for MSBuild had to be installed, which publishes the analysis in a dedicated SonarCloud project \cite{dev}.
The available statistics are very helpful for code reviews.\\

As a little extra, not only our Dafny language server part is analyzed by Sonar,
but the whole Dafny project.
From the bugs, vulnerabillities, code duplications and code smells revealed
by this analysis, the whole GitHub team working on the Dafny project can benefit. \\

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth-2cm]{sonarexamaple.png}
    \caption{Example of a Useful Sonar Finding}
    \label{fig:sonarexample}
\end{figure}


Unfortunately, the code coverage by tests is not analyzed.
Searching for an alternative, \textit{OpenCover} was found as a very common tool for code coverage analysis in \Csharp.
Unfortunately, it only runs under Windows  \cite{opencover}.
The CI server bases on Linux, though.
During our research we came across \textit{monocov} \cite{monocov}.
This tool would run under Linux and analyze .NET Framework projects.
Unfortunately, this project was archived and has not been maintained for almost 10 years \cite{monocov}.

Since we would not gain much value with sonar code coverage, we decided not to pursue this approach any further.
The coverage information is provided by the ReSharper extension \textit{dotCover} \cite{dotcover} to the developers.

\subsubsection{Client End-to-End Tests}
The end-to-end tests base on a lot of dependencies, such as a headless instance of Visual Studio Code.
In consultation with our supervisors, we have removed these tests from the client project and replaced them with own specially written integration tests on the server side.
This can be justified with the client only containing a minimal amount of logic that is required for the visual representation.
Any other logic was moved to the server.

\subsubsection{Static Program Analysis and Formatting for TypeScript}
During development it became apparent that it would be useful to check the correct formatting
for the TypeScript code from the client in the CI.
Various tools are used locally, which automatically format the syntax correctly when saved.
These tools are described in the developer documentation \cite{dev}.
However, if someone does not install these tools,
and the code is not formatted according to the definition,
the CI pipeline will fail. \\

Prettier was selected for this function.
Alternatively, ESLint would have been an option \cite{eslint}.
This would also have automatically integrated a static code analysis in the CI.
However, we decided to use a combination of SonarLint and Prettier,
so that the local analysis tool is matched to SonarQube \cite{dev}.

\subsubsection{Docker}
For an easier testability of the CI, we installed Docker locally.
This allowed us to resolve CI issues locally and platform-independently (through the Docker Client) in case of problems.
More details are stated in the developer documentation \cite{dev}.
The documentation found there helps future developers to easily and efficiently test changes to the docker container locally. \\
